
Привет! С вами Евгений. В этом уроке мы с вами поговорим о задаче регрессии, обсудим некоторые ее важные свойства, как она решается, почему она решается именно так и как интерпретировать то, что получается в результате. Если подумать, термин регрессия довольно странный. Кажется, что в нем есть что-то негативное. Впервые этот термин появился в конце XIX века в работе Френсиса Гальтона, которая называлась «Регрессия к середине в наследственности роста». В этой работе Френсис Гальтон исследовал зависимость между средним ростом детей и средним ростом их родителей и обнаружил, что отклонение роста детей от среднего составляет примерно две трети отклонения роста родителей от среднего. Этот результат контринтуитивен. Кажется, он означает, что с течением времени люди должны рождаться все ближе и ближе к среднему росту. На самом деле, естественно, этого не происходит. Чтобы лучше понять эффект регрессии к среднему, давайте посмотрим на другое творение Френсиса Гальтона, которое называется машина, или доска, Гальтона. Это механическая машина, в которой сверху в центральной части находятся шарики. Когда открывается заслонка, шарики начинают постепенно сыпаться вниз, ударяясь о штырьки, которые расположены на одинаковом расстоянии друг от друга. При каждом соударении шарика со штырьком вероятность того, что он упадет налево и направо от штырька, одинакова. Постепенно шарики начинают собираться в секциях внизу в до боли знакомую нам фигуру — в гауссиану, или плотность нормального распределения. Чтобы понять эффект регрессии к среднему, давайте мысленно подставим к машине Гальтона снизу еще одну такую же машину. Если теперь мы уберем перегородку, которая удерживает шарики в верхней половине, они начнут постепенно осыпаться вниз и сформируют внизу еще одну такую же гауссиану. Давайте теперь зафиксируем какой-то конкретный шарик, который в нижней половине находится в одной из ячеек близко к краю, и попытаемся понять, откуда сверху он мог в эту ячейку попасть. Оказывается, что с достаточно большой вероятностью этот шарик пришел не из ячейки, которая находится в верхней половине прямо над ячейкой, в которой он оказался внизу, а от ячейки ближе к середине. Это происходит просто потому, что в середине шариков больше. Эффект регрессии к среднему проявляется во многих практических задачах. Например, если вы дадите какой-то достаточно сложный тест группе студентов, то большую роль в том, насколько хорошо они его пройдут, будут играть не только их знания по предмету, но и то, насколько им повезло, то есть случайный фактор. Поэтому если вы изолируете, например, 10 % студентов, которые прошли тест лучше всех (набрали больше всего баллов) и дадите им еще одну версию этого теста и заставите их пройти его снова, средний балл в этой группе скорее всего упадет. Просто потому что люди, которым повезло в первый раз, скорее всего уже не будут так удачливы во второй. Это эффект регрессии к середине. Френсис Гальтон был достаточно плодовитым ученым. Он был основоположником дактилоскопии, исследовал явление синестезии, внес существенный вклад в метеорологию, впервые описав циклоны и антициклоны, а также, например, изобрел ультразвуковой свисток для собак. Но именно регрессия и по сей день остается одним из наиболее важнейших инструментов, к которому он приложил руку. Давайте начнем его изучение. Чаще всего под регрессией понимают минимизацию среднеквадратичной ошибки: квадратов отклонений откликов y от их предсказанных значений a(x). Поскольку минимизируется сумма квадратов отклонений, этот метод называется методом наименьших квадратов (сокращенно МНК). Для линейной регрессии, в которой мы приближаем отклик линейной комбинации наших факторов x с весами w, эта задача имеет аналитическое решение. Именно этим частично объясняется популярность среднеквадратичной ошибки. В XIX веке, когда эта задача впервые возникла, никакого способа ее решения, кроме аналитического, быть не могло. Сейчас мы можем минимизировать не только среднеквадратичную ошибку, но и, например, среднюю абсолютную, то есть сумму модулей отклонений нашей модели от отклика. Такая задача является частным случаем класса задач квантильной регрессии, о которых мы будем говорить подробно в следующих видео. Далее в этом уроке вас ждет знакомство с методом максимального правдоподобия, подробное изучение свойств регрессии, регуляризации, а также задача логистической регрессии. 

Прежде чем дальше изучать задачи регрессии, в этом видео мы познакомимся с методом максимизации правдоподобия — одним из мощнейших методов математической статистики. Представьте, что у вас есть некая случайная величина x и ее функция распределения F(x) зависит от неизвестного вам параметра θ. Пусть у вас есть выборка из этой случайной величины, то есть совокупность независимых, одинаково распределенных ее реализаций. Как по выборке лучше всего оценить неизвестный параметр θ? Чтобы понять метод максимального правдоподобия, давайте рассмотрим еще один исторический пример. Эти данные собраны в конце XIX века. В Генеральный штаб прусской армии ежегодно в течение 20 лет от десяти кавалерийских корпусов поступали данные о количестве смертей кавалеристов в результате гибели под ними коня. Эти данные — перед вами в таблице. Как видно, в большей части отчетов никто не умер, однако в 65-и отчетах умер один человек, в 22-х отчетах умерло два человека и так далее. Поскольку эта случайная величина — количество умерших кавалеристов — явно счетчик, логично попробовать моделировать ее распределением Пуассона. Но как выбрать неизвестный параметр λ для этого распределения? Давайте запишем функцию вероятности для распределения Пуассона. Вероятность того, что случайная величина из распределения Пуассона с параметром λ примет значение k, определяется вот такой величиной. Теперь вероятность получения значения, равного i-тому элементу выборки, записывается той же формулой. Поскольку наша выборка состоит из независимых, одинаково распределенных случайных величин, мы можем записать суммарную вероятность выборки, вероятность получения именно такой выборки, и она будет являться произведением вероятности каждого элемента этой выборки. Эта функция является функцией неизвестного параметра λ, обозначается за L, и называется правдоподобием выборки, то есть вероятностью получения именно такой выборки. Если теперь мы, в качестве нашей оценки λ, возьмем значение, которое максимизирует функцию правдоподобия, мы получим оценку, которая называется оценкой максимального правдоподобия. Логично оценивать λ именно таким способом, поскольку, выбирая именно такое λ, мы максимизируем вероятность получения именно таких данных, которые у нас есть. В рассматриваемой задаче несложно показать, что оценка максимального правдоподобия для параметра λ совпадает с выборочным средним. Чтобы это показать, нужно всего лишь взять логарифм от функции правдоподобия — логарифмирование не влияет на положение максимума этой функции, но превращает произведение вероятностей в сумму, с которой легче оперировать. После чего от этого логарифма нужно взять производную, приравнять ее к нулю, и таким образом найти точку максимума. Вы можете без труда проделать это упражнение. В данном случае выборочное среднее равно 0,61, то есть данные, которые мы рассматриваем, лучше всего моделировать случайной величиной с распределением Пуассона и параметром 0,61. Вот так в общем виде выглядит метод максимума правдоподобия. Трюк с логарифмированием, который я вам только что описал, используется достаточно часто, потому что оперировать с логарифмом правдоподобия действительно проще, чем с самим правдоподобием. Если вы имеете дело со случайной величиной из непрерывного распределения, метод максимального правдоподобия работает точно так же, за исключением того, что функция вероятности нашей случайной величины заменяется на ее плотность. Метод максимального правдоподобия обладает рядом очень полезных свойств. Во-первых, получаемые с его помощью оценки являются состоятельными, то есть при увеличении объема выборки они начинают стремиться к истинным значениям параметра θ. Во-вторых, они являются асимптотически нормальными, то есть опять же, с ростом объема выборки, оценки максимального правдоподобия все лучше описываются нормальным распределением с средним, равным истинному значению θ и дисперсией, равной величине, обратной к информации Фишера. Что это такое, совершенно не важно. Важно только, что эта величина также с успехом может быть оценена по выборке. Итак, в этом видео мы познакомились с методом максимизации правдоподобия — крайне мощным и полезным методом оценки неизвестных параметров распределения. Из следующего видео вы узнаете, при чем тут регрессия.

Теперь, когда вы знаете, что такое правдоподобие и зачем его максимизировать, давайте вернемся к задаче регрессии. Попробуем разобраться, что именно получается в результате минимизации среднеквадратичной ошибки. Строя регрессию, мы пытаемся значение отклика y приблизить нашей модельной функции a от факторов x. Это можно представить следующим образом. Значение отклика y представляет собой сумму регрессионной функции a(x) и компоненты ε, которая описывает некоторый случайный шум. Если этот случайный шум имеет нормальное распределение с нулевым средним и какой-то дисперсией σ², оказывается, что задача минимизации среднеквадратичной ошибки дает оценку максимального правдоподобия для регрессионной функции a(x). Казалось бы, какое это имеет значение? Дело в том, что опираясь на этот факт, мы можем использовать свойства метода максимального правдоподобия, в частности асимптотическую нормальность. Используя эту асимптотическую нормальность, мы можем определять значимость признаков xʲ в нашей модели и делать отбор этих признаков, а также мы можем строить доверительные интервалы для значения отклика на новых объектах, которых в нашей обучающей выборке нет. Распределение шума не обязательно должно быть нормальным, может быть каким-то другим. Например, можно попытаться описать его распределением Лапласа с нулевым средним. Формула для функции плотности вероятности такого распределения перед вами. Вот так выглядит ее график. По сравнению с нормальным распределением, распределение Лапласа имеет более тяжелые хвосты, то есть для него более вероятны большие значения ε. Если мы моделируем шум распределением Лапласа, мы разрешаем наблюдениям сильнее отклоняться от нашей модели, и за счет этого мы получаем решение, которое более устойчиво к выбросам. Оказывается, что если шум действительно описывается распределением Лапласа, то к оценке максимального правдоподобия приводит минимизация средних абсолютных отклонений. Итак, из этого видео вы узнали, что регрессия методом наименьших квадратов дает оценку максимального правдоподобия в случае нормального шума, а регрессия со средней абсолютной ошибкой дает оценку максимального правдоподобия для нашей регрессионной функции, если шум лапласовский. В следующем видео вы узнаете, как регрессию можно интерпретировать как оценку среднего.

Продолжим разбираться с тем, что же такое получается в качестве ответа в задаче регрессии. Начнем снова с метода наименьших квадратов. Разберемся со среднеквадратичной ошибкой. Чтобы разбираться было проще, давайте сделаем некоторые упрощающие предположения. Пусть для начала у нас нет никаких признаков x, а a — это просто константа. Пусть кроме того у нас есть бесконечная выборка из y, то есть фактически и не выборка вовсе, а полностью известно распределение случайной величины y. Пусть оно задается плотностью f (t). В таком случае среднеквадратичная ошибка принимает следующий вид. Нетрудно показать, раскрыв квадрат под знаком интеграла и продифференцировав полученное выражение, что минимум такому функционалу доставляет математическое ожидание y. То есть наилучшая константа, которая аппроксимирует значение y в смысле среднеквадратичной ошибки — это математическое ожидание. Пусть теперь a — это не константа, а некоторая произвольная функция от наших признаков x. Можно показать, что в этом случае минимумом среднеквадратичной ошибки является условное математическое ожидание y по x. То есть среднее значение y при таких x. Теперь, если мы имеем дело с конечной выборкой, получается, оценка, которую мы получаем, минимизируя среднеквадратичную ошибку — это наша лучшая аппроксимация условного математического ожидания. Если регрессия линейная, то есть отклик y моделируется линейной комбинацией наших признаков x с весами w, то w*, минимизирующее среднеквадратичную ошибку, задает наилучшую линейную аппроксимацию условного математического ожидания. В каком-то смысле этот результат интуитивно понятен. Пусть, например, y = 2. Поскольку среднеквадратичная ошибка будет симметрична относительно 2, мы будем одинаково штрафовать наши модельные предсказания a (x) за большие отклонения от 2 как в большую, так и в меньшую сторону. Неудивительно, что минимизируя симметричную функцию потерь, мы получаем в ответе какое-то среднее. Однако оказывается, что условное математическое ожидание доставляет минимум не только среднеквадратичной ошибке, но и более широкому классу функций потери, которые называются дивергенциями Брегмана. Дивергенции Брегмана порождаются любой непрерывной дифференцируемой выпуклой функцией φ. Среднеквадратичная ошибка является ее частным случаем. Таким образом, минимизируя любую дивергенцию Брегмана, мы получаем какую-то оценку для условного математического ожидания. И вот это уже довольно странно, потому что в семействе дивергенций Брегмана можно найти функции, которые относительно y несимметричны. Они могут выглядеть вот так или так или так, то есть они сильнее штрафуют за отклонение нашей модели от y в какую-то из сторон. Тем не менее, наилучшей оценкой является все еще условное математическое ожидание. Этот результат достаточно контринтуитивен, и получен он был не так давно. А вот средняя абсолютная ошибка в семейство дивергенций Брегмана не входит. Минимизируя вот эту среднюю абсолютную ошибку, график которой представляет собой такой треугольник, мы получаем тоже оценку какого-то среднего, но другого. Это уже оценка не условного математического ожидания, а условной медианы y|x. Треугольник, описывающий среднюю абсолютную ошибку, можно попробовать наклонить в какую-то из сторон на угол τ. Минимизируя такой функционал, мы получаем оценку для условного квантиля y|x порядка τ. τ, естественно, меняется от 0 до 1, поскольку это квантиль. Итак, в этом видео мы узнали, что решение задачи регрессии наименьших квадратов представляет собой наилучшую возможную по выборке оценку условного матожидания y при условии x. Решение задачи квантильной регрессии дает оценку условного квантиля y|x. А при использовании средней абсолютной ошибки мы получаем оценку условной медианы med (y|x). Далее в программе: регуляризация.

В этом видео мы поговорим о регуляризации линейных регрессионных моделей. Как вы видели ранее в этом курсе, регрессионные модели имеют свойство переобучаться. Если вы взяли слишком сложную модель и у вас недостаточно данных для того, чтобы точно определить ее параметры, вы легко можете получить какую-то модель, которая будет очень хорошо описывать вашу обучающую выборку, но при этом очень плохо обобщаться на тестовую. Бороться с этим можно разными способами. Можно попробовать взять больше данных. Имея много данных, вы сможете точнее оценить вашу модель и уменьшить переобучение. Очень часто это решения недоступно, поскольку дополнительные данные стоят дополнительных денег. Даже в задачах, когда, казалось бы, у вас есть терабайты данных, например в задачах веб-поиска, эффективный объем выборки зачастую часто оказывается очень маленьким, если, например, мы хотим показывать для каждого пользователя его персонализированные результаты. Мы вынуждены использовать только его историю. Еще один способ борьбы с переобучением — это упрощение модели. В частности, можно, например, взять просто меньше признаков. Какие-то из признаков просто выбросить. Для этого нужно перебрать большое количество подмножеств наших признаков xj-тое, и общее количество подмножеств, которые нужно перебрать, очень быстро растет с ростом размерности задачи. Полный перебор часто оказывается недоступен. Кроме того, если признаков действительно много и они сильно зашумлены, может оказаться, что в выборке находится какая-то пара признаков, которые на обучении очень похожи. В этом случае совершенно непонятно, какой из этих признаков следует взять в модель. Наконец, еще один способ борьбы с переобучением линейной модели — это ограничение весов у признаков. Вы видели ранее в курсе, что когда линейная модель переобучается, веса у признаков становятся большими по модулю и разными по знаку. Ограничивая значение этих весов по модулю, можно с переобучением до какой-то степени побороться. Мы рассматривали два способа регуляризации: L2-регуляризатор добавляет к функционалу потерь слагаемое, равное сумме квадратов весов нашей линейной модели с множителем λ; L1-регуляризатор использует вместо суммы квадратов сумму модулей весов. Регрессия с L2-регуляризатором называется ридж-регрессией или гребневой регрессией, а с L1-регуляризатором — лассо. Очень важно, что константное слагаемое в регуляризатор входить не должно. Штрафуя за большое значение константы, переобучение мы не уменьшим, а вот качество моделей и на обучающей, и на тестовой выборке упадет очень сильно. Чтобы понять, чем отличаются L1 и L2-регуляризаторы давайте рассмотрим простой модельный пример. Пусть матрица «объекты-признаки» X — квадратная, диагональная и единичная, то есть на ее диагонали стоят единицы, а вся остальная часть заполнена нулями. В этом случае решение метода наименьших квадратов дает вектор весов w со звездочкой, j-тая компонента которого равна yj-тому. Если мы делаем гребневую регрессию, то j-тая компонента w уменьшается в (1 + λ) раз. Если мы делаем лассо, формула для j-той компоненты вектора весов более сложная. Давайте посмотрим на графики. На графиках показана зависимость j-той компоненты оптимального вектора весов w со звездочкой от yj-того. Если мы минимизируем среднеквадратичную ошибку, не используя регуляризаторы, то эта зависимость единичная, то есть wj-тое и yj-тое всегда одинаковые. На графиках — это пунктирная диагональная линия. Если мы использует регуляризацию L2, зависимость wj-тое со звездочкой все еще линейная, но веса прижаты к нулю. Лассо делает кое-что более интересное. Оптимальные веса лассо также прижимаются к нулю, однако в середине на этом графике появляется интервал размером λ, в котором веса обращаются в ноль в точности. То есть если у нас значению отклика маленькое, то вес получается нулевым. Именно поэтому лассо отбирает признаки. Если у признаков низкая предсказательная способность, в лассо они получают нулевой вес и таким образом из модели исключаются. С другой стороны, посмотреть на регуляризацию можно, если рассмотреть, как устроена ошибка регрессии. Давайте посмотрим на матожидание квадрата этой ошибки. Оно представляет собой сумму трех компонент. Первая компонента — это квадрат смещения, то есть квадрат разности между математическим ожиданием регрессионной модели, оцениваемой по выборке, и истинной неизвестной нам регрессионной модели. Вторая компонента — это дисперсия нашей выборочной оценки. А третья — это дисперсия шума, на который повлиять мы никак не можем. Метод наименьших квадратов дает оценки, которые имеют нулевое смещение. Однако, используя регуляризацию, мы можем получить оценки, у которых матожидание квадрата ошибки меньше за счет того, что дисперсия у них может быть меньше, несмотря а то, что эти оценки смещенные. Чтобы лучше понять баланс между смещением и дисперсией, представьте, что вы стреляете по мишеням. Среднее количество очков, которое вы при этом набираете, определяется двумя величинами: во-первых, средним облака точек, которое образуют результаты ваших выстрелов; во-вторых, разбросом выстрелов относительно этого среднего, то есть дисперсией. Естественно, больше всего очков вы получите, если вы будете стрелять точно и в цель. В этом случае у вас может быть какое-то небольшое смещение и маленькая дисперсия. Переобучение в линейных моделях приводит к тому, что вы стреляете с цель, но не точно. Смещения у вас нет, но дисперсия очень большая. Часто оказывается, что можно набрать больше очков, если вы будете стрелять не совсем в цель, но более точно. Именно это позволяет сделать использование регуляризации в линейных моделях. В байесовской статистике гребневая регрессия соответствует заданию нормального априорного распределения на коэффициенты линейной модели, а метод лассо — заданию Лапласовского априорного распределения на коэффициенты. Подробнее о байесовской статистике вы узнаете из гостевого видео, которое вас ждет в конце этого урока. Задача гребневой регрессии имеет аналитическое решение. К матрице X транспонированное X, которая обращается в методе наименьших квадратов, вы добавляете диагональную матрицу, у которой на диагонали стоят значения λ — веса при регуляризаторе. Для решения задачи лассо аналитического решения не существует. Однако есть очень эффективный численный способ получения решения, поэтому методом лассо тоже можно прекрасно пользоваться. Итак, в этом видео мы поговорили про регуляризацию как один из способов борьбы с переобучением линейных регрессионных моделей. Регуляризация приводит к тому, что вы получаете смещенные оценки коэффициентов модели, но суммарная ошибка таких моделей может быть меньше за счет того, что оценки коэффициентов имеют меньшую дисперсию. Это справедливо и для L1, и L2-регуляризации, однако про L1-регуляризацию мы еще выяснили, что она отбирает признаки, обнуляя веса у некоторых коэффициентов, и разобрались в том, почему так происходит. В следующем видео мы поговорим про логистическую регрессию.

Из этого видео вы узнаете, что такое логистическая регрессия, для чего она нужна и как работает. Логистическая регрессия – это метод обучения с учителем. Имея обучающую выборку по признаковому описанию объектов, вы пытаетесь предсказать значение отклика. Единственное отличие от задач линейной регрессии, которые мы рассматривали до этого, в том, что значение отклика у нас бинарное, то есть y принимает значение 0 и 1. На первый взгляд кажется, что это задача бинарной классификации, ее можно решать своими методами, которые мы рассматривали до этого в этом курсе. Однако оказывается, что линейная регрессия в этой задаче тоже может быть полезна. Если мы будем просто минимизировать среднеквадратичную ошибку между откликом y и линейной комбинацией факторов x, мы получим вектор весов w*, и можем мы его использовать следующим образом: если значение линейной комбинации факторов на объекте больше, чем 1/2, мы будем предсказывать, что наш объект будет относиться к классу 1; если меньше 1/2, то он будет относиться к классу 0. Этот метод называется методом линейного дискриминанта Фишера, и это один из самых старых методов классификации. На самом деле, мы можем хотеть предсказывать не просто метки классов на наших объектах, а вероятности того, что объекты относятся к одному из классов. Обозначим условную вероятность того, что y = 1 при условии x, за π(x). Вот эту функцию π(x) мы и хотим как-то оценить. Можно попробовать делать это с помощью обычной линейной регрессии. На первый взгляд кажется, что эта идея не самая плохая. Дело в том, что π(x), поскольку y – величина бинарная, совпадает с условным матожиданием, y при условии x. Это внушает нам надежду, что обычная минимизация методом наименьших квадратов может дать какую-то хорошую оценку. Проблема здесь заключается в том, что получаемая линейная комбинация факторов не обязательно лежит на отрезке от 0 до 1. Представьте, что вы предсказываете вероятность невозврата платежа по кредитной карте в зависимости от размера задолженности. Если на такие данные вы настроите линейную регрессию, вы получите, что при задолженности в 2000 долларов вероятность того, что клиент просрочит платеж по кредиту, составляет примерно 0,2. С другой стороны, вероятность того, что клиент просрочит платеж по кредиту при задолженности в 500 долларов, равна нулю. Это немного странно. Еще более странно, если задолженность составляет меньше 500 долларов, вероятность просрочки отрицательная. Если клиент должен больше 10000 долларов, вероятность больше 1. Это очень странный результат, совершенно непонятно, как его интерпретировать. Решить эту проблему можно следующим образом. Давайте возьмем какую-то функцию g, которая переводит интервал от 0 до 1 на множество всех действительных чисел, и построим оценку не для условного матожидания, как мы привыкли в линейной регрессии, а для функции g от этого условного матожидания, или, что то же самое, условное матожидание мы будем приближать обратной функцией к g, к g в минус первой, от нашей линейной комбинации факторов. Такое семейство моделей в статистике называется обобщенными линейными моделями. В задаче бинарной классификации в качестве функции g в минус первой берется функция, которая выглядит следующим образом: это сигмоида. Для одномерного случая значение w0, константа, определяет положение центра сигмоиды на числовой оси, а w1, вес при единственном факторе, определяет форму этой сигмоиды. Если вес w1 положительный, то сигмоида возрастающая, если отрицательный, то убывающая. Чем больше по модулю значение w1, тем круче наклон сигмоиды в области ее середины. Если мы возьмем сигмоиду и построим логистическую регрессию в задаче с вероятностью невозврата кредита, мы получим что-то более вменяемое. Наша вероятность будет принимать значение от 0 до 1, как мы и хотели. Кроме того, полезное свойство заключается в том, что изменение на краях диапазона значений признака x приводит к меньшим изменениям вероятности, которую мы моделируем. Это логично, изменение в плюс-минус 100 долларов при размере задолженности около 2000 приводит к большим изменениям вероятности просрочки платежа по кредитной карте, а изменение в плюс-минус 100 долларов при размере задолженности около 500 – к небольшим изменениям. Чтобы получить саму функцию g обобщенной линейной модели, мы произведем несложные арифметические преобразования выражения для π(x), которое мы записали до этого, и получим следующую функцию. Дробь, которая стоит здесь под логарифмом, представляет собой отношение вероятности того, что y = 1, к вероятности того, что y = 0, вероятности эти условные по x. Это отношение называется риском. Вместе с логарифмом это выражение называется логит. Именно поэтому метод называется логистической регрессии, потому что мы приближаем логит линейной комбинации наших факторов. Как эту модель можно настраивать? Давайте будем делать это методом максимизации правдоподобия. Запишем выражение для правдоподобия обучающей выборки и сразу для удобства возьмем от правдоподобия логарифм. Если мы поставим перед логарифмом знак минус, то получившуюся функцию мы будем не максимизировать, а минимизировать. Это немного более привычно, поскольку мы привыкли минимизировать функции потери в задачах регрессии. Такой функционал называется еще log-loss, или кросс-энтропия, у него много названий. Если мы переобозначим нулевой класс за минус первый, то путем несложных преобразований можно получить логистическую функцию потерь, которую вы уже встречали до этого. Задача максимизации правдоподобия в логистической регрессии очень хорошо решается числами, поскольку эта функция выпуклая, она имеет единственный глобальный максимум. Кроме того, мы можем очень хорошо оценивать ее градиент и гессиан. Проблемы возникают, только если наши объекты разных классов линейно разделимы в пространстве признаков. Представьте, например, что в вашей обучающей выборке все клиенты, задолженность которых составляет меньше 1300 долларов, платеж вовремя вернули, а все клиенты, задолженность которых больше 1400, платеж не вернули. В этом случае максимизация правдоподобия приводит к тому, что значение веса w1 при нашем признаке уходит в бесконечность. Вместо сигмоиды мы получаем вот такую ступеньку. Это плохо, поскольку мы переобучаемся на нашу обучающую выборку. Чтобы решить эту проблему, можно использовать методы регуляризации, о которых мы говорили до этого: можно использовать как l1, так и l2 регуляризацию. Вероятности, которые дает логистическая регрессия, можно использовать для классификации, для предсказания итоговых меток классов. Для этого нужно на вероятность, которую мы оцениваем, в каком-то месте поставить порог p0, и при значении вероятности выше порога предсказывать метку y = 1, а ниже порога – y = 0. Интуитивно кажется, что лучше всего выбирать порог = 1/2. На самом деле этот порог p0 можно подбирать для каждой задачи отдельно так, чтобы обеспечить оптимальный баланс между точностью и полнотой классификатора. Итак, в этом видео мы поговорили о логистической регрессии, мы узнали, что это регрессионный метод, позволяющий предсказывать вероятность того, что y = 1, по каким-то факторам x. В логистической регрессии используется линейная модель для логита, для логарифма отношения вероятностей y = 1 при условии x и y = 0 при условии x. Оценка параметров логистической регрессии делается методом максимального правдоподобия. Вот и все. Далее в программе вас ждет знакомство с некоторыми важными техническими трюками, которые часто используются при настройке линейных моделей.

Линейные модели: статистический
взгляд
5.1. Задача регрессии
5.1.1. История термина
Впервые термин регрессия появился в конце XIX века в работе Френсиса Гальтона:
В этой работе «Регрессия к середине в наследственности роста» Френсис Гальтон исследовал зависимость
между средним ростом детей и средним ростом их родителей и обнаружил, что отклонение роста детей от
среднего составляет примерно 2/3 отклонения роста родителей от среднего. Казалось бы, со временем люди
должны рождаться все ближе и ближе к среднему росту. На самом деле, естественно, этого не происходит.
1
Лучше понять эффект регрессии к среднему позволяет другое творение
Френсиса Гальтона, которое называется машина, или доска, Гальтона.
Это механическая машина, в которой сверху в центральной части находятся шарики. Когда открывается заслонка, шарики начинают постепенно
сыпаться вниз, ударяясь о штырьки, которые расположены на одинаковом
расстоянии друг от друга. При каждом соударении шарика со штырьком вероятности того, что он упадет налево и направо от штырька, равны. Постепенно шарики начинают собираться в секциях внизу в гауссиану, или плотность нормального распределения.
Чтобы понять эффект регрессии к среднему, давайте мысленно подставим к машине Гальтона снизу еще
одну такую же машину. Если теперь убрать перегородку, которая удерживает шарики в верхней половине, они
начнут постепенно осыпаться вниз и сформируют внизу еще одну такую же гауссиану. Если зафиксировать
какой-то конкретный шарик в нижней половине ближе к краю, то откажется, что с достаточно большой
вероятностью этот шарик пришел не из ячейки, которая находится в верхней половине прямо над ячейкой,
в которой он оказался внизу, а от ячейки ближе к середине. Это происходит просто потому, что в середине
шариков больше.
Эффект регрессии к среднему проявляется во многих практических задачах. Например, если дать студентам очень сложный тест, большую роль в том, насколько хорошо они его пройдут, будут играть не только
их знания по предмету, но и везение, то есть случайный фактор. Поэтому, если изолировать 10% студентов,
которые прошли тест лучше всех (набрали больше всего баллов) и дать им еще один вариант теста, то средний балл в этой группе скорее всего упадет. Просто потому что люди, которым повезло в первый раз, скорее
всего уже не будут так удачливы во второй — в этом и состоит эффект регрессии к середине.
Френсис Гальтон был основоположником дактилоскопии, исследовал явление синестезии, внес существенный вклад в метеорологию, впервые описав циклоны и антициклоны, а также, например, изобрел ультразвуковой свисток для собак. Но именно регрессия и по сей день остается одним из наиболее важнейших
инструментов, к которому он приложил руку.
5.1.2. Регрессия
Чаще всего под регрессией понимают минимизацию среднеквадратичной ошибки: квадратов отклонений откликов y от их предсказанных значений a(x).
Q(w, X) = 1
`
X
`
i=1
(hw, xii − yi)
2
−1.5 −1 −0.5 0 0.5 1 1.5
0
0.5
1
1.5
2
2.5
a(xi) − yi
Q(a, x)
Поскольку минимизируется сумма квадратов отклонений, этот метод называется методом наименьших
квадратов (сокращенно МНК). Для линейной регрессии задача имеет аналитическое решение.
w∗(x) = argminw Q(w, X) = (XT X)
−1XT
y.
2
Именно этим частично объясняется популярность среднеквадратичной ошибки.
В XIX веке, когда эта задача впервые возникла, никакого способа ее решения, кроме аналитического, быть
не могло. Сейчас можно численно минимизировать не только среднеквадратичную ошибку, но и, например,
среднюю абсолютную, то есть сумму модулей отклонений нашей модели от отклика:
Q(a, X) = 1
`
X
`
i=1
|a(xi) − yi
| , a∗(x) = argmina Q(a, X).
−1.5 −1 −0.5 0 0.5 1 1.5
0
0.5
1
1.5
a(xi) − yi
Q(a, x)
Такая задача является частным случаем класса задач квантильной регрессии.
5.2. Метод максимизации правдоподобия
Пусть x — некоторая случайная величина с функцией распределения F(x, θ), которая зависит от неизвестного
параметра θ, а Xn = (X1, ..., Xn) — выборка размера n, сгенерированная из распределения F(x, θ). Необходимо
оценить по данной выборке неизвестный параметр.
5.2.1. Метод максимизации правдоподобия: пример
Чтобы понять метод максимального правдоподобия, можно рассмотреть еще один исторический пример. Эти
данные собраны в конце XIX века. В Генеральный штаб прусской армии ежегодно в течение 20 лет от десяти
кавалерийских корпусов поступали данные о количестве смертей кавалеристов в результате гибели под ними
коня.
Кол-во погибших 0 1 2 3 4 5 Всего
Кол-во донесений 109 65 22 3 1 0 200
Поскольку данная случайная величина — счетчик, ее необходимо моделировать распределением Пуассона,
функция вероятности которого имеет вид:
P(X = k) = λ
k
e
−λ
k!
Поскольку выборка состоит из независимых, одинаково распределенных случайных величин, вероятность
получения строго определенной выборки равна произведению вероятностей получения каждого из элементов
этой выборки:
P(Xn
, λ) = Yn
i=1
λ
Xi e
−λ
Xi
!
≡ L(Xn
, λ).
3
Функция L зависит от неизвестного параметра λ и называется правдоподобием выборки. В качестве оценки
для λ можно взять такое значение, которое максимизирует функцию правдоподобия:
λˆ
ОМП = argmaxλ L(XN , λ)
Эта оценка называется оценкой максимального правдоподобия.
Несложно показать, что в рассматриваемой задаче оценка максимального правдоподобия для параметра
λ совпадает с выборочным средним:
λˆ
ОМП = X¯
n = 0.61
Чтобы это сделать, можно взять производную от логарифма функции правдоподобия и приравнять ее к нулю.
Здесь используется тот факт, что логарифмирование L не меняет точку максимума функции.
5.2.2. Метод максимизации правдоподобия: общий вид
В общем виде метод максимума правдоподобия записывается следующим образом. Пусть некоторая случайная величина x имеет распределение F(x, θ), Xn — выборка размера n:
X ∼ F(x, θ), Xn = (X1, ..., Xn),
Тогда функция правдоподобия имеет вид:
L(Xn
, λ) = Yn
i=1
P(X = Xi
, θ).
Поскольку при логарифмировании не меняются положения максимумов функции, удобно работать не с самим
правдоподобием, а с логарифмом правдоподобия:
lnL(Xn
, λ) = Xn
i=1
ln P(X = Xi
, θ).
Оценкой максимального правдоподобия называется величина:
λˆ
ОМП = argmaxλ
lnL(XN , λ)
В случае непрерывной случайной величины метод максимального правдоподобия записывается аналогично:
X ∼ F(x, θ), L(Xn
, λ) = Yn
i=1
f(X = Xi
, θ), λˆ
ОМП = argmaxλ L(XN , λ).
5.2.3. Свойства метода максимального правдоподобия
Метод максимального правдоподобия обладает рядом очень полезных свойств:
• Состоятельность, то есть получаемые оценки при увеличении объема выборки начинают стремиться к
истинным значениям:
λˆ
ОМП → θ при n → ∞.
• Асимптотическая нормальность, то есть с ростом объема выборки, оценки максимального правдоподобия все лучше описываются нормальным распределением с средним, равным истинному значению θ, и
дисперсией, равной величине, обратной к информации Фишера:
λˆ
ОМП ∼ N

θ, I−1
(θ)

при n → ∞.

5.3. Регрессия как максимизация правдоподобия
5.3.1. Модель шума: нормальное распределение
При решении задачи регрессии значение отклика приближается в виде
y = a(x) + ε,
где a(x) — регрессионная функция, а компонента ε описывает случайный шум.
Если этот случайный шум имеет нормальное распределение с нулевым средним и дисперсией σ
2
, оказывается, что задача минимизации среднеквадратичной ошибки
a∗ = argmina
1
`
X
`
i=1
(a(xi) − yi)
2
дает оценку максимального правдоподобия для регрессионной функции a(x).
Этот факт позволяет использовать в задаче с регрессии свойства метода максимального правдоподобия.
Например, используя асимптотическую нормальность, можно определять значимость признаков x
j в модели
и делать их отбор. Также можно строить доверительные интервалы для значения отклика на новых объектах,
которых нет в обучающей выборке.
5.3.2. Модель шума: распределение Лапласа
Распределение шума не обязательно должно быть нормальным и может быть каким-то другим. Например,
можно попытаться описать его распределением Лапласа с нулевым средним. Формула для функции плотности
вероятности такого распределения:
f(x) = α
2
e
−α|x|
.
−4 −2 2 4
0.1
0.2
0.3
0.4
0.5
x
f(x)
Нормальное
Лапласа
Рис. 5.1: График функции Лапласа с нулевым средним и нормального распределения.
По сравнению с нормальным распределением, распределение Лапласа имеет более тяжелые хвосты, то
есть для него более вероятны большие значения ε. Другими словами, если моделировать шум распределением
Лапласа, то наблюдения могут сильнее отклоняться от выбранной модели. За счет этого получается решение,
которое более устойчивое к выбросам. Оказывается, что если шум действительно описывается распределением
Лапласа, то к оценке максимального правдоподобия приводит минимизация средних абсолютных отклонений:
a∗ = argmina
1
`
X
`
i=1
|a(xi) − yi
|.
5
5.4. Регрессия как оценка среднего
5.4.1. Среднеквадратичная ошибка
Пусть для начала a — константа (что соответствует ситуации, когда отсутствуют признаки), а y является
случайной функцией с плотностью распределения f(t). В таком случае среднеквадратичная ошибка имеет
вид:
Q(a) = Z
t
(a − t)
2
f(t)dt.
Нетрудно показать, что:
a∗ = argmina Q(a) = Ey,
то есть наилучшая константа, которая аппроксимирует значение y в смысле среднеквадратичной ошибки —
это математическое ожидание.
Если a(x) — произвольная функция признаков x, функционал среднеквадратичной ошибки имеет вид:
Q(a, X) = Z
t
(a(x) − t)
2
f(t)dt,
а его минимум будет доставлять условное математическое ожидание:
a∗(x) = argmina Q

a(x)

= E(y|x).
Таким образом, в случае с конечной выборкой:
Q(a(x), X) = 1
`
X
`
i=1
(a(xi) − yi)
2
,
оценка, получаемая при минимизации среднеквадратичной ошибки:
a∗(x) = argmina Q(a, X)
является лучше аппроксимацией условного математического ожидания E(y|x). В случае линейной регрессии,
то есть когда отклик моделируется линейной комбинацией hw, xii:
Q(w, X) = 1
`
X
`
i=1
(hw, xii − yi)
2
, w∗ = argminw Q(w, X),
выражение hw∗, xii является наилучшей линейной аппроксимацией условного математического ожидания E(y|x).
Полученный результат согласуется с интуитивными представлениями. Действительно, пусть yi = 2, график зависимости ошибки на этом объекте в зависимости от предсказания алгоритма a(x) выглядит следующим образом.
1 2 3 4
a(xi)
0
yi = 2
Рис. 5.2: Зависимость ошибки от предсказания алгоритма в случае среднеквадратичной ошибки.
По графику видно, что одинаково штрафуются отклонения предсказания как в большую, так и в меньшую сторону от истинного значения yi
. Поэтому не удивительно, что функция, которая доставляет минимум
функции, представляет собой какое-то среднее.

5.4.2. Дивергенции Брегмана
Однако оказывается, что условное математическое ожидание доставляет минимум не только среднеквадратичной ошибки, но и более широкого класса функций потерь, которые называются дивергенциями Брегмана.
Дивергенции Брегмана порождаются любой непрерывной дифференцируемой выпуклой функцией ϕ:
Q(a, X) = ϕ(y) − ϕ(a(X)) − ϕ
0
(a(X))(y − a(X)).
Среднеквадратичная ошибка является частным случаем дивергенции Брегмана. Минимизируя любую дивергенцию Брегмана, мы получаем оценку для условного математического ожидания:
a∗ = argmina Q(a, X) — лучшая аппроксимация E(y|x).
1 2 3 4
a(xi)
0
yi = 2
1 2 3 4
a(xi)
0
yi = 2
Рис. 5.3: Несколько функции потерь из класса дивергенций Брегмана.
Этот результат уже является несколько странным, поскольку в семействе дивергенций Брегмана можно найти, в том числе, несимметричные относительно y функции. Такие функции больше штрафуют за отклонение
модели в большую или меньшую сторону. Это результат может быть несколько контринтуитивным и получен
не так давно.
5.4.3. Средняя абсолютная ошибка и несимметричная абсолютная ошибка
Средняя абсолютная ошибка:
Q(a, X) = 1
`
X
`
i=1
|a(xi) − yi
|
не входит в семейство дивергенций Брегмана. При ее минимизации получается оценка не условного математического ожидания, а оценка условной медианы:
a∗ = argmina Q(a, X) — лучшая аппроксимация med(y|x).
1 2 3 4
a(xi)
0
yi = 2
1 2 3 4
a(xi)
0
yi = 2
Рис. 5.4: Графики симметричной средней абсолютной и несимметричной абсолютной функций ошибок.
7
Несимметричная абсолютная функция ошибки («несимметричность» определяется параметром τ ):
Q(a, X) = 1
`
X
`
i=1

(τ − 1)
yi < a(xi)

+ τ

yi ≥ a(xi)
 (yi − a(xi))
имеет в некотором смысле наклоненный график по сравнению с графиком симметричной абсолютной ошибки.
При минимизации такого функционала получается лучшая оценка для соответствующего условного квантиля:
a∗ = argmina Q(a, X) — лучшая аппроксимация y|x порядка τ.
5.5. Регуляризация
5.5.1. Переобучение регрессионных моделей
Если используется слишком сложная модель, а данных недостаточно, чтобы точно определить ее параметры,
эта модель легко может получиться переобученной, то есть хорошо описывать обучающую выборку и плохо
— тестовую. Бороться с этим можно различными способами:
• Взять больше данных. Такой вариант обычно недоступен, поскольку дополнительные данные стоят
дополнительных денег, а также иногда недоступны совсем. Например, в задачах веб-поиска, несмотря на наличие терабайтов данных, эффективный объем выборки, описывающей персонализированные
данные, существенно ограничен: в этом случае можно использовать только историю посещений данного
пользователя.
• Выбрать более простую модель или упростить модель, например исключив из рассмотрения некоторые признаки. Процесс отбора признаков представляет собой нетривиальную задачу. В частности, не
понятно, какой из двух похожих признаков следует оставлять, если признаки сильно зашумлены.
• Использовать регуляризацию. Ранее было показано, что у переобученной линейной модели значения
весов в модели становятся огромными и разными по знаку. Если ограничить значения весов модели, то
с переобучением можно до какой-то степени побороться.
5.5.2. L1-регуляризация и L2-регуляризация
Есть несколько способов провести регуляризацию:
• L2-регуляризатор (ridge-регрессия или гребневая регрессия):
w∗ = argminw


1
`
X
`
i=1
(hw, xii − yi)
2 + λ
X
d
j=1
w
2
j

 .
• L1-регуляризатор (lasso-регрессия или лассо-регрессия):
w∗ = argminw


1
`
X
`
i=1
(hw, xii − yi)
2 + λ
X
d
j=1
|wj |

 .
Понять различия между L1 и L2 регулязаторами можно на модельном примере. Пусть матрица «объекты–
признаки» X является единичной матрицей размера ` × `:
X =


1 0 ... 0
0 1 ... 0
.
.
.
.
.
.
.
.
.
.
.
.
1 1 ... 1


.
Тогда при решении задачи линейной регрессии использование метода наименьших квадратов без регуляризации:
w∗ = argminw
X
`
i=1
(wi − yi)
2
,

дает следующий вектор весов:
w∗j = yj
При использовании гребневой регуляризации (L2–регуляризация) компоненты вектора весов имеют вид:
w∗j =
yj
1 + λ
,
а при использовании L1–регуляризатора (lasso):
w∗j =



yj − λ/2, yj > λ/2
yj + λ/2, yj < −λ/2
0, |yj | ≤ λ/2.
При использовании только МНК без регуляризации w∗j = yj . Соответствующая линия изображена пунктиром на обоих графиках. При использовании L2 регуляризации зависимость w∗j от yj все еще линейная,
компоненты вектора весов ближе расположены к нулю.
yj
w∗j
L2–регуляризация
yj
w∗j
L1–регуляризация
Рис. 5.5: Зависимость w∗j от значения отклика yj при использовании различных регуляризаторов.
В случае L1 регуляризации график выглядит несколько иначе: существует область (размера λ) значений
yj , для которых wj = 0. То есть lasso, или L1–регуляризация, позволяет отбирать признаки, а именно: веса
признаков, обладающих низкой предсказательной способностью, оказываются равными нулю.
5.5.3. Смещение и дисперсия
Можно показать, что математическое ожидание квадрата ошибки регрессии представляет собой сумму трех
компонент:
E (a∗(x) − y)
2 = (Ea∗(x) − a(x))2
| {z }
Квадрат смещения
+ Da∗(x)
| {z }
Дисперсия оценки
+ σ
2
|{z}
Шум
.
От выбора модели зависит квадрат смещения и дисперсия оценки, но не шум, который является свойством
данных, а не модели.
Метод наименьших квадратов дает оценки, которые имеют нулевое смещение. Регуляризация позволяет
получить смещенные оценки с меньшим E (a∗(x) − y)
2
за счет того, что у этой оценки будет меньше дисперсия.
Следующая аналогия позволяет лучше понять баланс между смещением и дисперсией. При стрельбе по
мишени среднее число набранных очков зависит от положения средней точки попадания и разбросом относительно этого среднего.
9
небольшая большая
Дисперсия
небольшое большое
Смещение
Рис. 5.6: Дисперсия и смещения при различных характерах стрельбы.
Лучший результат будет, если стрелять без смещения и без разброса. Переобучению линейных моделей
соответствует стрельба без смещения, но с огромным разбросом. И часто оказывается, что можно набрать
больше очков, стреляя не совсем в цель, то есть со смещением, но зато более точно. Именно это и позволяет
добиться регуляризация.
5.5.4. Решение задач гребневой регрессии и лассо
В байесовской статистике гребневая регрессия соответствует заданию нормального априорного распределения
на коэффициенты линейной модели, а метод лассо — заданию Лапласовского априорного распределения.
Подробнее о байесовской статистике написано в соответствующем уроке.
Задача гребневой регрессии имеет аналитическое решение:
w∗ =

XT X + λI−1
XT
y.
Для решения задачи лассо аналитического решения не существует, однако есть очень эффективный численный способ получения решения.
5.6. Логистическая регрессия
5.6.1. Логистическая регрессия
Пусть X — пространство объектов, Y — пространство ответов, X = (xi
, yi)
`
i=1 — обучающая выборка, x =
(x
1
, ..., xd
) — признаковое описание.
Логистическая регрессия – это метод обучения с учителем в задаче бинарной классификации Y = {0, 1}.
1
5.6.2. Метод линейного дискриминанта Фишера
Метод линейного дискриминанта Фишера, один из самых старых методов классификации, заключается в
минимизации среднеквадратичной ошибки:
Q(w, X) = 1
`
X
`
i=1
(hw, xii − yi)
2
.
В результате получается вектор весов:
w∗ = argmina Q(w, X),
причем, если для некоторого объекта hw, xii > 0.5, объект относится к первому классу y = 1, в ином случае
— к нулевому y = 0. На самом деле, хочется предсказывать не просто метки классов, а вероятности того, что
объекты относятся к какому-то из классов:
P(y = 1|x) ≡ π(x).
Хотя π(x) совпадает с условным математическим ожиданием E(y|x):
π(x) = 1 · P(y = 1|x) + 0 · P(y = 0|x) = E(y|x),
использовать для оценки вероятности обычную линейную регрессию
π(x) ≈ hw, xi
не получится: получаемая линейная комбинация факторов не обязательно лежит на отрезке от 0 до 1.
Пусть, например, решается следующая задача. Необходимо предсказать вероятность невозврата платежа
по кредитной карте в зависимости от размера задолженности.
Рис. 5.7: Применение линейной регрессии в задаче оценки вероятности просрочки платежа.
По обучающей выборке была настроена модель линейной регрессии. Получается, что при задолженности
2000$ вероятность просрочить платеж по кредиту равна 0.2, при задолженности 500$ — нулю, а при меньших
значениях и вовсе отрицательная. Также, если задолженность больше 10000$, вероятность просрочки будет
больше 1. Не понятно, как интерпретировать этот результат.
5.6.3. Обобщенные линейные модели
Пусть функция g : (0, 1) 7→ R переводит интервал (0, 1) на множество всех действительных чисел, тогда
можно решать задачу линейой регрессии:
g (E(y|x)) ≈ hw, xi,
в которой строится оценка не для условного матожидания E(y|x), а для g (E(y|x)). Что то же самое:
E(y|x) ≈ g
−1
(hw, xi)
11
В статистике такое семейство моделей называется обобщенными линейными моделями.
В задаче бинарной классификации в качестве g
−1 используется сигмоида:
π(x) ≈
e
hw,xi
1 + e
hw,xi
.
В одномерном случае значение параметр w0 сигмоиды определяет положение её центра на числовой оси, а
w1 — форму это сигмоиды:
• Если w1 > 0, сигмойда возрастающая:
0
0.2
0.4
0.6
0.8
1
x
π(x)
• Если w1 < 0, сигмоида убывающая:
0
0.2
0.4
0.6
0.8
1
x
π(x)
Чем больше по модулю значение w1, тем круче наклон сигмоиды в области ее середины.
5.6.4. Предсказание вероятностей
Если использовать сигмоиду:
π(x) ≈
e
hw,xi
1 + e
hw,xi
в обобщенной линейной модели в задаче логистической регрессии, результат будет более адекватным:
• Вероятность π(x) ∈ [0, 1], как и требуется.
12
• На краях области значений x функция (вероятность) π(x) слабо меняется при небольших изменениях
x, когда как существенно изменяется, если x находится в середине диапазона своих значений.
Рис. 5.8: Применение логистической регрессии в задаче оценки вероятности не вернуть задолженность.
Последнее свойство является весьма полезным. Например, в уже рассмотренной задаче при размере задолженности в районе 2000$ оценка вероятности просрочки платежа сильно изменяется при увеличении или
уменьшении задолженности на 100$. С другой стороны при размере задолженности в 500$ увеличение задолженности на 100$ приводит только к незначительным изменениям требуемой оценки.
5.6.5. Оценка параметров
По функции π(x) можно восстановить функцию g, которая фигурирует в определении обобщенной линейной
модели:
π(x) ≈
e
hw,xi
1 + e
hw,xi ⇐⇒ hw, xi ≈ ln
Риск
z }| {
π(x)
1 − π(x)
| {z }
Логит
.
Отношение, стоящее под логарифмом, называется риском, а весь логарифм называется «логит». Именно
поэтому метод называется логистической регрессией: логит приближается линейной комбинацией факторов.
Настройка модели происходит методом максимизации правдоподобия L(X). Удобнее однако не максимизировать правдоподобие, а минимизировать минус логарифм от правдоподобия:
− lnL(X) = −
X
`
i=1

yi
ln π(xi) + (1 − yi)ln(1 − π(xi))
Такой функционал также имеет названия log-loss, кросс-энтропия и другие.
Если изменить метку нулевого класса на −1, то получится логистическая функция потерь в таком виде,
в котором она встречалась в курсе до этого:
Q(w, X) = X
`
i=1
ln (1 + exp (−yihw, xi))
5.6.6. Решение задачи максимизации правдоподобия
Задача максимизации правдоподобия в логистической регрессии очень хорошо решается численно, поскольку
правдоподобие — выпуклая функция, а следовательно, она имеет единственный глобальный максимум. Кроме
того, ее градиент и гессиан могут быть хорошо оценены.
Если объекты из разных классов линейно разделимы в пространстве признаков, возникает проблема переобучения: сигмоида вырождается в «ступеньку».
13
Рис. 5.9: Проблема переобучения в задаче логистической регрессии.
Например, такая ситуация возникает, если в уже упомянутой задаче оценки вероятности вернуть задолженность обучающая выборка такова, что все клиенты с задолженностью менее 1300$ вернули платеж
вовремя, а все клиенты с задолженностью более 1300$ — нет.
В этом случае максимизация правдоподобия приводит к тому, что kwk → ∞. В таких случаях необходимо
использовать методы регуляризации, например L1 или L2 регуляризатор.
5.6.7. Предсказание отклика
Вероятности, которые дает логистическая регрессия, можно использовать для классификации, то есть для
предсказания итоговых меток классов. Для этого выбирается порог p0 и объект относится к классу 1 только
в случае π(x) > p0. В остальных случаях объект относится к классу 0.
Порог p0 не следует выбирать всегда равным 0.5, как это может показаться из интуитивных соображений. Его необходимо подбирать для каждой задачи отдельно таким образом, чтобы обеспечить оптимальный
баланс между точностью и полнотой классификатора.
