

Меня зовут Евгений Соколов, и я рад приветствовать вас на курсе «Обучение на размеченных данных». Это второй курс специализации «Машинное обучение и анализ данных», в котором мы начнем знакомиться, собственно, с машинным обучением. Центральной темой этого курса является обучение с учителем. На самом деле, мы немного затрагивали эту тему, когда говорили про интерполяцию в прошлом курсе. Интерполяция — это значит восстановление функции по нескольким точкам, в которых известны ее значения. Обучение с учителем — это тоже восстановление общей закономерности по конечному числу примеров. Хотя постановки задач похожи, у них есть много отличий в том, как они решаются и какие требования выдвигаются к решению. Мы будем обсуждать эти различия в нашем курсе. Давайте разберем не очень сложный пример, на котором поймем, в чем заключается обучение с учителем и машинное обучение. Представьте, что у нас есть некоторый сайт про фильмы, на который можно зайти, найти страницу нужного фильма, почитать про него, когда он снят, кто в нем играет, какой бюджет был у этого фильма и, возможно, даже купить его и посмотреть. И есть некоторый пользователь, который заходит на наш сайт, находит страницу нужного ему фильма, читает и задается вопросом, смотреть или не смотреть, интересен ему этот фильм или не интересен. И мы хотим понять это за него. Как это можно сделать? Есть несколько подходов к решению. Подход первый, самый глупый — это дать пользователю посмотреть этот фильм. Понятно, что он потратит 1,5–2 часа, фильм может не понравится, он будет недоволен. Подход второй — показывать ему случайную рекомендацию. То есть говорить понравится или не понравится просто из генератора случайных чисел. Подход тоже не самый лучший: пользователь может смотреть фильм, он опять ему не понравится, и он будет недоволен нашим сайтом. Наконец, можно пригласить психолога-киномана разрешить ситуацию. Этот человек оценит пользователя, поймет, что ему нравится, а что — нет, оценит фильм, вспомнит про него всё и поймет, кому он может нравиться, сопоставит эту информацию и выдаст пользователю рекомендацию, смотреть или не смотреть. Этот подход довольно сложный. Скорее всего, таких специалистов не очень много в мире, и будет сложно отмасштабировать это на миллионы пользователей нашего сайта. Но на самом деле это и не нужно. Давайте поймем, что у нас много примеров, много ситуаций, когда другие пользователи заходили на страницу фильмов, принимали решение посмотреть фильм и дальше ставили оценку, по которой можно понять, понравилось им или не понравилось. Это примеры, та информация, из которой можно восстановить общую зависимость. В этом и заключается задача машинного обучения. Давайте введем пару обозначений, которыми мы будем пользоваться в нашем курсе. Машинное обучение — это раздел математики, поэтому в нем, конечно же, есть место формулам. Объектом будем называть то, для чего нужно сделать предсказание. В нашем примере объектом является пара, состоящая из пользователя и фильма, и для нее нужно предсказать, понравится ли этот фильм этому пользователю. Объекты будем обозначать маленькой буквой x. Далее, пространство объектов — это множество всех возможных объектов, для которых может понадобиться делать предсказание. В нашем случае это множество всех возможных пар «пользователь-фильм». Пространство объектов будем обозначать буквой x красивое. Ответом будем называть то, что нужно предсказать. В нашем случае ответ — это понравится пользователю фильм или не понравится. Обозначать ответы будем маленькой буквой y. Наконец, пространство ответов — это множество всех возможных ответов, с которыми мы можем работать. В нашем примере это множество состоит из двух элементов: −1 и +1. −1 означает, что пользователю фильм не понравился, +1 означает, что пользователю фильм понравился. Обозначать пространство ответов будем буквой y красивое. Как мы уже выясняли с вами в прошлом курсе, объекты — это сущности из реального мира, а компьютер не понимает, что это такое, он не знает, что такое пользователь или фильм, ему нужно объяснить эти объекты с помощью чисел, которые компьютер уже может понимать. Признак — это некая числовая характеристика объекта, а совокупность всех признаков, которых d штук, называется признаковым описанием объекта. Кстати, хотя я говорю, что признак — это число, мы с вами увидим, что есть и другие случаи, когда признак — это элемент множества, или строка, или что-то еще, но всё это — нечто, понятное компьютеру. Признаковое описание — это d-мерный вектор и можно с ним работать как с вектором, складывать, умножать на числа и так далее, то есть это — некий объект линейной алгебры. В нашем примере признаки могут быть самые разные: прошлые оценки этого пользователя другим фильмам; его анкетные данные; оценки, которые другие пользователи ставили этому фильму и так далее. Центральным понятием машинного обучения является обучающая выборка. Это то, это те примеры, на основе которых мы будем строить общую закономерность. Обучающая выборка обозначается большой буквой X и состоит из l пар объектов и ответов. xi-тое — это i-тый объект обучающей выборки, yi-тое — это истинный ответ на нем, то, что нужно предсказать. Иногда отдельный большой вопрос — это как собрать обучающую выборку, откуда ее взять. В нашем случае это довольно просто: она будет состоять из прошлых событий, когда пользователь оценивал какой-то фильм. Наконец, нам нужно что-то, что будет делать предсказания, что-то, что поможет нам решать нашу задачу с фильмами и пользователями. Это называется алгоритмом или моделью и обозначается a(x). Это, по сути, функция, которая переводит объекты в ответы, которая отображает пространство объектов в пространство ответов. a(x) принимает на вход, собственно, объект x. Кстати, слово «алгоритм» обозначает, что эта функция должна быть легко реализуема на компьютере, что ее должно быть легко использовать в системах машинного обучения. Простым примером алгоритмов являются линейные алгоритмы, о которых мы тоже говорили в прошлом курсе. Идея линейных алгоритмов очень простая: давайте возьмем все признаки и сложим их с некоторыми весами, и еще прибавим некоторую, некоторый константный коэффициент w0. Поскольку такая линейная комбинация признаков — это, по сути, любое вещественное число, а в нашей задаче ответов всего два, −1 и +1, понравился фильм или не понравился, то нужно взять знак от этой суммы, то есть для задачи классификации, для задачи определения понравится фильм или не понравился, алгоритм будет иметь вид знака от линейной комбинации всех признаков объекта. Давайте поймем, что не все алгоритмы одинаково подходят для решения нашей задачи. Например, рассмотрим константный алгоритм a(x) = 1. Алгоритм, который для всех пар «пользователь-фильм» говорит, что фильм этому пользователю понравится. Понятно, что это довольно бесполезный алгоритм, который вряд ли принесет пользу нашему сайту, поэтому нам нужно ввести некоторую характеристику полезности, характеристику качества алгоритма для данной конкретной задачи. Эта характеристика называется функционалом ошибки и обозначается как Q. Q принимает на вход алгоритм и выборку и возвращает некоторую характеристику того, насколько хорошо работает данный алгоритм на данной выборке. В нашем случае это может быть, например, доля неправильных ответов, то есть берем всю выборку x и смотрим, на какой доле пар «пользователь- фильм» наш алгоритм ошибся, выдал неправильное предсказание. Понятно, что чем меньше будет такая доля неверных ответов, тем лучше. Обратите внимание на еще один факт: функция Q называется функционалом ошибки, а не функцией. Функционал, потому что она принимает на вход другую функцию, алгоритм является функцией, как вы помните. Итак, задача обучения состоит в подборе такого алгоритма a, на котором достигается минимум функционала ошибки. Сразу возникает вопрос: а из какого множества нужно выбирать лучший алгоритм? Для этого вводится понятие семейства алгоритмов, которое обозначается буквой A красивое. По сути, это множество всех алгоритмов, среди которых мы будем искать лучший, тот, который лучше всего подходит для решения нашей задачи, которая составляет минимум к функционалу ошибки. Простейшим примером семейства алгоритмов являются решающие пни. Каждый решающий пень делает очень простую вещь: он берет некоторый один фиксированный признак xj-тое и сравнивает его значение на данном объекте с некоторым порогом t. Если значение признака меньше этого порога, то алгоритм возвращает ответ −1, говорит, что этому пользователю фильм не понравится. Если же значение j-того признака больше или равно порога t, то алгоритм дает ответ +1, говорит, что пользователю фильм понравится. Это очень простые алгоритмы, они могут проверять только очень простые факты вроде «Пользователь посмотреть больше трех комедий». Понятно, что здесь нужно проверять более сложные, парные взаимодействия, например, «Пользователь посмотрел больше трех комедий» и «Данный фильм является комедией». Но решающие пни на это не способны, для этого нужны более сложные семейства алгоритмов. Тем не менее, решающие пни пригодятся нам в этом курсе для составления сложных композиций алгоритмов. Обратите внимание на одно обозначение, которое используется на этом слайде. Это квадратные скобки, скобки Айверсона или нотация Айверсона. Внутри скобок Айверсона находится некое логическое выражение, например, «значение j-того признака меньше порога t». Если это выражение — верное, то значение скобок равно 1, если же значение неверное, то значение скобок равно 0. Итак, чтобы заниматься машинным обучением, нужно уметь отвечать на три вопроса: как измерять качество, какой функционал ошибки использовать в данной задаче; какое взять семейство алгоритмов, из чего выбирать оптимальный алгоритм для данной задачи; и, наконец, как этот выбор производить, как делать обучение алгоритма? При этом есть ряд других вопросов, которые не менее важны, но относятся скорее к анализу данных, а не к машинному обучению. Анализ данных — это более широкая область науки, которая включает в себя множество различных эвристик, очень полезных для этой работы. Например, как сформировать признаки, какие выбрать признаки, чтобы на них задача решалась лучше всего, или как готовить признаки, как их предобрабатывать, как они должны выглядеть, чтобы алгоритм хорошо обучался на них, или, например, какую выбрать метрику, чтобы алгоритм не только хорошо настраивался, но и приносил реальную экономическую пользу заказчику. В этом уроке мы поговорим о совсем базовых понятиях: о постановках задач машинного обучения и о том, какие признаки бывают машинного обучения. Это очень простые и базовые вещи, которые пока не имеют отношения к реальным задачам, но нам очень важно о них поговорить, чтобы использовать общую терминологию, говорить на одном языке, а уже в следующем уроке мы перейдем к реальным примерам и к реальным семействам алгоритмов, а именно к линейным.

В этом видео мы поговорим о том, какие бывают типы задач обучения на размеченных данных или обучения с учителем, и обсудим несколько их примеров. В прошлый раз мы обсуждали общую постановку задачи обучения с учителем. В ней есть обучающая выборка, то есть набор пар «объект и ответ» — объектов и ответов, которые нужно предсказывать для этих объектов. И нужно найти такой алгоритм из семейства алгоритмов A (красивое), на котором будет достигаться минимум функционала ошибки, то есть найти такой алгоритм, который будет лучше всего решать нашу задачу, лучше всего подходить к нашей обучающей выборке. В зависимости от того, какие именно ответы должны возвращать алгоритмы в этой задаче, зависит, с каким типом задачи мы имеем дело. Иными словами, тип задачи определяется пространством ответов, которое мы обозначали Y (красивое). Замечу, что бывают и другие задачи, не только обучения с учителем, но об этом в следующем видео. А первый пример, о котором мы поговорим, это задача бинарной классификации. В этих задачах пространство ответов состоит из всего двух элементов, их обычно обозначают как 0 и 1 или –1 или +1. Множества объектов, которые относятся... которые имеют один ответ, например ответ «–1», называются классом, и говорят, что нужно уметь относить объект к одному из двух классов или классифицировать эти объекты. Давайте рассмотрим простой пример. Если у нас каждый объект описывается всего двумя признаками, то есть выборка двумерная, то можно эту выборку нарисовать. По одной оси отложим значение первого признака, по другой — значение второго признака, и каждая точка в этих осях будет обозначать один объект обучающей выборки. По сути, задача классификации состоит в том, чтобы провести некоторую разделяющую кривую, которая будет отсекать один класс от другого, разделять синие и красные точки. Примеров задачи бинарной классификации очень много. Например, можно предсказывать, понравится ли пользователю фильм — то, о чем мы уже говорили. Или, например, вернет ли клиент кредит или не вернет — задача кредитного скоринга, очень популярная в банковской сфере. Или, например, нужно ли делать пациенту операцию, будет ли операция иметь долгосрочный положительный эффект. Или можно просто предсказывать, качественное ли вино, сделано ли оно по всем канонам или это дешевая подделка. Классов может быть не два, а больше. Задача, в которой конечное число классов, например K штук, называется многоклассовой классификацией. Визуально это означает следующее. Допустим, признаков все еще два, но при этом цветов точек (а цвет обозначает класс точки, класс объекта) будет больше. В этом случае надо провести не одну разделяющую кривую, а много. Для каждого класса будет своя кривая, которая отсекает этот класс от всех остальных. Понятно, что это уже более сложная задача. Какие есть примеры задач многоклассовой классификации? Например, можно пытаться понять, из какого сорта винограда сделано вино. Понятно, что сортов конечное количество, значит это многоклассовая классификация. Или, например, можно определять тематику научной статьи. Из какой области эта статья? Она про математику, про физику, про биологию или, может быть, про философию? Или, например, можно пытаться понять по фотографии, какой тип машины там присутствует: мотоцикл, легковая или грузовая машина? Это может понадобиться, чтобы автоматически определять, какую плату за проезд по платной дороге взять с автомобилиста по фотографии его машины возле КПП. Классов может быть не конечное число. Если классов бесконечное количество, например ответом может быть любое вещественное число, то мы имеем дело с задачей регрессии. Собственно, в задачах регрессии пространство ответов — это все вещественные числа. Давайте разберем простой пример. Нам нужно предсказать рост человека по его весу. В этом случае по оси x мы отложим вес человека в килограммах — признак, по оси y отложим ответ — рост человека в сантиметрах. Каждая точка будет соответствовать одной паре «объект–ответ». В нашем примере очень легко видеть, что зависимость почти линейная. Можно провести прямую, которая будет очень хорошо предсказывать рост человека по его весу. Есть и более сложные примеры задач регрессии. Например, предсказание температуры на завтрашний день. Понятно, что температура — это вещественное число. Или, например, предсказание прибыли магазина в следующем году, или определение возраста человека по его фотографии. Еще одним примером задачи обучения с учителем является задача ранжирования. Это довольно тяжелая задача, о которой мы не будем говорить в этом курсе, но знать о ней очень полезно. Это задача, с результатом решения которой вы сталкиваетесь каждый день, когда ищете что-то в поисковике, например в Яндексе. Ранжирование поисковой выдачи заключается в следующем. Пользователь вводит некоторый запрос. Например, ему хочется найти картинки с котятами. И у нас есть множество всех страниц в Интернете, которые нам известны. Это миллиарды или даже триллионы страниц. И нужно отсортировать все эти страницы по тому, насколько они подходят под запрос пользователя, насколько они отвечают на его вопрос. Понятно, что очень непросто отсортировать, отранжировать такое количество документов, но эта задача вполне решаемая. Итак, мы обсудили основные постановки задач обучения с учителем. Это бинарная многоклассовая классификация, это регрессия. Также мы немножко поговорили о ранжировании. А в следующем видео поговорим о задачах обучения без учителя.

В этом видео мы поговорим о том, какие бывают постановки задач в машинном обучении помимо обучения с учителем. И рассмотрим несколько примеров задач обучения без учителя. Итак. Обучением с учителем называются такие задачи, в которых у нас есть и объекты, и истинные ответы на них. И нужно по этим парам восстановить общую зависимость, построить алгоритм или модель, которые будут предсказывать ответы по объектам. Задача обучения без учителя — это такая задача, в которой есть только объекты, ответов нет, и при этом с этими объектами нужно что‐то сделать. Также есть и промежуточные постановки. Например, частичное обучение. В этом случае у нас есть объекты, но ответы известны лишь на части объектов. И нужно как‐то, имея эту информацию, тоже восстановить общую зависимость, построить модель. Или, например, активное обучение. Это задача, в которой есть объекты, но получать ответ для объекта, истинный ответ, очень дорого, очень тяжело. Поэтому алгоритм должен уметь определять, на каких объектах ему надо знать ответ, чтобы лучше всего обучиться, построить наилучшую модель. В этом видео мы обсудим 3 примера постановки задачи обучения без учителя, чтоб вы понимали важность этого класса задач. Первым примером будет задача кластеризации. В этом случае у нас есть некий набор объектов, и нужно сгруппировать их, найти группы похожих объектов. У этой задачи есть 2 проблемы. Проблема первая: мы даже зачастую не знаем количество этих групп, мы не знаем, сколько кластеров имеется в наших данных. А во‐вторых, мы не знаем правильных ответов, мы не знаем истинные кластеры, которые нужно выделять. Поэтому задача решается очень тяжело, здесь нельзя измерить точно качество решения. Кстати, вот этим она и отличается от задачи классификации. В классификации тоже нужно относить объект к одной из групп, но там есть примеры объектов этих групп. Поэтому задача классификации гораздо проще, в ней можно померить качество решения. Примеров задачи кластеризации очень много. Например, эта сегментация пользователей, например интернет‐магазина или мобильного оператора. Им зачастую интересно найти группы похожих пользователей, чтобы дальше, например, заниматься маркетингом для каждой группы в отдельности. Понять, что такого особенного в этой группе, что все пользователи в ней схожие, и ориентировать рекламу именно на этот сегмент, на эту группу. Или, например, можно искать группы похожих пользователей социальных сетей. Но при этом кластеризовать — группировать — можно не только людей. Например, можно кластеризовать гены, пытаясь найти такие группы генов, которые одновременно включаются или выключаются у разных людей в разных условиях. Второй пример задачи обучения без учителя — это задача визуализации. Здесь нам нужно нарисовать многомерную выборку — выборку, которая описывается большим числом признаков. То есть надо уметь многомерную точку отразить в двумерное пространство, то есть на плоскость, или в трёхмерное пространство, то есть в пространство. При этом отобразить нужно так, чтобы визуализация, изображение нашей выборки в двумерном или трёхмерном пространстве отражало структуру исходной многомерной выборки. Чтобы глядя на это изображение, можно было понять, как устроены эти данные. что с ними можно делать. Также обычно есть требование, чтобы эта визуализация была красивой, чтоб на неё было приятно смотреть. Классическим примером задачи визуализации является визуализация data set'а MNIST. Это data set, в котором были отсканированы рукописные начертания всех цифр — от 0 до 9. Понятно, что каждый скан, каждое изображение, характеризуется сотнями пикселей. Но при этом, если грамотно отразить эту многомерную выборку на плоскость, то цифры вполне будут группироваться. Например, цифра 0 будет отдельным облаком где‐то. Причём особенностью хорошей визуализации будет то, что даже начертания одной и той же цифры будут разделяться на разные группы в зависимости от того, как именно написана эта цифра, например с засечкой или без. Третий пример задачи обучения без учителя — это задача обнаружения аномалий, поиска аномалий. В ней требуется обнаруживать, определять, что данный объект не похож на все остальные, что он является аномальным. При этом при обучении у нас есть только примеры обычных, неаномальных объектов, а примеров аномальных либо нет вообще, либо настолько мало, что невозможно воспользоваться классическими методами обучения с учителем. При этом задача очень важная. Например, можно пытаться обнаружить что в самолёте есть поломка по показателям сотен датчиков, расположенных в нём. Такое обнаружение позволит избежать аварии, понятно, что это очень полезно. Или, например, если у нас есть интернет‐сайт, например интернет‐магазин или поисковый сайт, можно пытаться, опять же, по многим показателям понять, что произошла поломка, аномалия, что с сайтом нужно что‐то делать, нужно его срочно чинить. Или, например, если есть некоторая модель машинного обучения, которая делает прогнозы, скажем, понравится ли пользователю фильм или нет, можно пытаться следить за ней, понимать, хорошо ли он делает предсказания, или что‐то поломалось. Например, из‐за того, что распределение одного из признаков поменялось. Итак, мы обсудили 3 примера постановки задач обучения без учителя: кластеризацию, визуализацию и поиск аномалий. В этом курсе мы не будем о них говорить, им будет посвящен следующий курс — «Поиск структуры в данных», приходите. А в следующем видео мы поговорим о том, какие бывают признаки в задачах машинного обучения.

В этом видео мы поговорим о признаках машинного обучения. Существует несколько классов, несколько типов признаков, и у всех свои особенности. Все нужно по-разному готовить и по-разному учитывать в алгоритмах машинного обучения. В этом видео мы больше поговорим о терминологии, чтобы закрепить ее, а о самих особенностях работы с этими признаками будем говорить в следующих уроках. Также мы немного затронем вопросы, какие проблемы могут встретиться в тех или иных видах признаков. Итак, как вы уже знаете, признак — это некоторое число или другая понятная компьютеру сущность, которая как-то описывает объект в доступной форме. Множество значений j-го признака будем обозначать буквой Dj. И первый тип признаков, о которых мы поговорим, это бинарные признаки. Это самый простой тип признаков, которые принимают значение 0 или 1. Всего два значения. Например, в задаче кредитного скоринга мы можем смотреть, клиент получает зарплату выше, чем в среднем по городу, или нет? Если ответ на вопрос «да, его зарплата выше, чем средняя», то значение признака равно 1, если ответ «нет», то значение признака равно 0. У него два значения, он является бинарным. Или другой пример: если мы классифицируем изображение фруктов, пытаясь понять, какой именно фрукт там нарисован, то мы можем сделать признак, который отвечает на вопрос: фрукт зеленый или нет? 1 — если зеленый, 0 — если нет. Тоже бинарный признак. Чуть более сложный класс признаков — это вещественные. В этом случае множество значений — это все вещественные числа. Например, в задаче кредитного скоринга это может быть возраст клиента — понятно, что это вещественное число. Или в задаче оценивание стоимости квартиры можно рассматривать признак «площадь квартиры». Или же в задаче предсказания оттока клиентов мобильного оператора можно смотреть на признак «количество звонков в колл-центр за последний месяц». Следующий класс признаков — категориальные. В случае с категориальными признаками множество значений — это некоторое неупорядоченное множество. Это означает, что мы можем сравнивать элементы этого множества лишь на равенство, на совпадение, но при этом нельзя сравнивать их между собой на больше или меньше. Простой пример категориального признака — это цвет глаз человека. Понятно, что есть несколько вариантов цвета глаз, и их нельзя сравнивать. Нельзя сказать, что зеленый цвет больше или меньше, чем голубой. Разумеется, можно вдариться в подробности и, например, сравнивать цвета по длине волны, но не факт, что во всех задачах такое сравнение имеет смысл. Еще один пример категориального признака — это город, где родился клиент банка, который сейчас просит дать ему кредит. Или еще один пример — это образование. Понятно, что образование может пойти по нескольких веткам, например высшее образование и среднее профессиональное, и их тоже нельзя сравнивать между собой. При этом опять же в некоторых задачах можно ввести осмысленный порядок на этих значениях, но об этом чуть позже. Категориальные признаки очень трудны в обращении. До сих пор появляются способы учета этих признаков в тех или иных методах машинного обучения. Частным случаем категориальных признаков являются порядковые признаки. В этом случае множество значений признака — это некоторое множество, которое является упорядоченным. То есть можно сравнивать значения между собой, но нельзя измерять расстояние между ними. Этим порядковые признаки отличаются от вещественных. Пример порядкового признака — это роль в фильме. Актер может иметь роль первого плана, роль второго плана, роль в массовке, и эти виды ролей легко сравниваются между собой. Или, например, тип населенного пункта — это тоже порядковый признак. Есть вполне определенный порядок на всех типах населенных пунктов: деревня, город, областной центр, столица — что-то в этом духе. Как я уже говорил, на образовании можно ввести порядок в некоторых задачах. Например, банк в задаче кредитного скоринга может ввести порядок на типах образования в зависимости от того, клиенты с каким образованием лучше или хуже возвращают кредиты. И, например, может получиться, что два высших образования хуже, чем одно высшее образование. Наконец, последний тип признаков на сегодня — это множествозначные признаки. Множествозначный признак — это такой признак, значение которого на объекте — это подмножество некоторого множества. Например, можно рассматривать множество всех фильмов, которые когда-либо вышли (это, наверное, сотни тысяч фильмов), и значение признака для одного посетителя сайта — это множество фильмов, которые он посмотрел, это подмножество множества всех фильмов. Или, например, в задачах анализа текстов если мы рассматриваем текст, например, сообщения в Twitter, один объект — это некоторое сообщение, некоторый набор слов, который является подмножеством Большого словаря. Давайте теперь поговорим о двух проблемах, с которыми можно столкнуться при работе с признаками. Первая из них — это выброс. Выбросом называется такой объект, значение признака на котором отличается от значений признака на большинстве объектов. Например, на этом графике нарисованы распределения некоторого признака, и видно, что у большинства объектов значение этого признака концентрируется вокруг 5, но при этом на одном или двух объектах оно равно 15. Эти объекты — выбросы. Значения на них не вписываются в общее распределение. При этом если вы будете настраивать алгоритм машинного обучения на такой признак, у него, скорее всего, будут проблемы. Он будет пытаться хорошо работать и с распространенными значениями признака, и с выбросами. Но при этом поскольку выбросы, скорее всего, приходят из другого распределения, они описываются другими закономерностями, у алгоритма могут возникнуть большие проблемы при этом. Иногда выбросы лучше просто выкинуть. Дело может быть даже не в выбросах, а в том, как распределен признак. Например, в задаче кредитного скоринга можно посмотреть на признак «город, где родился клиент». При этом из каких-то городов у банка будет много клиентов, например из Москвы или Питера, а из каких-то небольших городов с населением в несколько десятков тысяч может быть мало клиентов — один или два. В этом случае статистики по этим городам будет слишком мало, чтобы накопить какие-то данные, чтобы делать выводы на основе того, что клиент пришел из этого города. С этим тоже могут быть проблемы, и надо как-то их решать. Мы будем говорить об этом, когда будем обсуждать работу с категориальными признаками. Если же признак вещественный, то проблема может быть в его распределении следующая: представьте, что мы смотрим на распределение стоимости книг в интернет-магазине, который торгует книгами. Оно будет выглядет примерно, как на этой гистограмме. У большинства книг цена будет не очень большая — несколько сотен рублей, и здесь почти все распределение концентрируется. Но при этом есть ряд книг, которые довольно дорогие — стоят тысячу или несколько десятков тысяч рублей. Они составляют «хвост» распределения, и он является довольно тяжелым. При работе с таким распределением, если алгоритм будет настраиваться на него, у него тоже могут возникнуть проблемы. Здесь нужно либо работать по отдельности с большинством книг, у которых цена вписывается в несколько сотен, и с дорогими книгами, или же как-то преобразовывать распределение этого признака, чтобы оно было более нормальным. Итак, мы обсудили основные типы признаков: это бинарные; вещественные; категориальные и порядковые; и множествозначные. Поговорили и о том, какие примеры этих признаков могут быть. А также обсудили две проблемы с признаками: они могут иметь выбросы, значение которых надо лучше выкинуть, а также могут быть проблемы в самом распределении признака, например слишком редкие значения или перекошенное распределение. На этом вводные лекции заканчиваются, и вам предстоит сделать задание по программированию, где вы посмотрите на реальные данные и заметите, какие могут быть проблемы в этих данных. А в следующем уроке мы уже начнем говорить о о реальных алгоритмах машинного обучения, а именно о линейных моделях.

1.1. Знакомство с машинным обучением
Приветствуем Вас на курсе «обучение на размеченных данных». Это второй курс специализации «Машинное
обучение и анализ данных», в котором начинается знакомство собственно с машинным обучением. Центральной темой этого курса является обучение с учителем. На самом деле эта тема была уже затронута в прошлом
курсе, когда речь шла про интерполяцию. Интерполяция — задача восстановления функции по нескольким
точкам, в которых известны ее значения.
Обучение с учителем — тоже восстановление общей закономерности по конечному числу примеров. Хотя
постановки задач похожи, у них есть много отличий в том, как они решаются и какие требования выдвигаются
к решению. Эти различия будут обсуждаться в данном курсе.
1.1.1. Пример: понравится ли фильм пользователю
Для начала будет рассмотрен не очень сложный пример, на котором можно понять, в чем заключается суть
обучения с учителем и машинного обучения. Пусть есть некоторый сайт, посвященный кино, на который
можно зайти, найти страницу нужного фильма, прочитать информацию про него: когда он снят, кто в нем
играет и какой бюджет у этого фильма, а также, возможно, купить его и посмотреть. Пусть есть некоторые
пользователи, которые находят страницу нужного фильма, читают и задаются вопросом «смотреть или нет?».
Необходимо понять, понравится ли пользователю фильм, если выдать ему рекомендацию о фильме. Есть
несколько подходов к решению:
• Подход первый, самый глупый — дать пользователю посмотреть этот фильм.
• Второй подход — дать случайный ответ и показать случайную рекомендацию. В обоих случаях пользователь может быть разочарован фильмом и он будет недоволен сайтом.
• Третий подход — пригласить психолога-киномана, чтобы разрешить ситуацию. Этот человек оценит
пользователя, оценит фильм и поймет, понравится ли этот фильм этому пользователю, сопоставив
информацию. Этот подход довольно сложный. Скорее всего таких специалистов не очень много, и будет
сложно отмасштабировать это решение на миллионы пользователей сайта. Но на самом деле это не
нужно.
Существует множество примеров — ситуаций, когда другие пользователи заходили на страницы фильмов,
принимали решение посмотреть фильм и далее ставили оценку, по которой можно понять, понравился им
фильм или нет. Задача машинного обучения состоит в восстановлении общей закономерности из информации
в этих примерах.
1.1.2. Основные обозначения
В рамках данного курса будут использоваться следующие обозначения: x — объект, X — пространство объектов, y = y(x) — ответ на объекте x, Y — пространство ответов.
Объектом называется то, для чего нужно сделать предсказание. В данном примере объектом является
пара (пользователь, фильм). Пространство объектов — это множество всех возможных объектов, для ко1
торых может потребоваться делать предсказание. В данном примере это множество всех возможных пар
(пользователь, фильм).
Ответом будет называться то, что нужно предсказать. В данном случае ответ — понравится пользователю фильм или нет. Пространство ответов, то есть множество всех возможных ответов, состоит из двух
возможных элементов: -1 (пользователю фильм не понравился) и +1 (понравился).
Признаковым описанием объекта называется совокупность всех признаков:
x = (x
1
, x2
, . . . , xd
).
Признак - это число, характеризующее объект. Признаковое описание является d-мерным вектором.
1.1.3. Выборка, алгоритм обучения
Центральным понятием машинного обучения является обучающая выборка X = (xi
, yi)
`
i=1. Это те самые примеры, на основе которых будет строиться общая закономерность. Отдельная задача — получение обучающей
выборки. В вышеупомянутом случае yi - это оценка фильма пользователем.
Предсказание будет делаться на основе некоторой модели (алгоритма) a(x), которая представляет из себя
функцию из пространства X в пространство Y. Эта функция должна быть легко реализуема на компьютере, чтобы ее можно было использовать в системах машинного обучения. Примером такой модели является
линейный алгоритм:
a(x) = sign(w0 + w1x
1 + . . . + wdx
d
).
Операция взятия знака sign берется ввиду того, что пространство Y состоит из двух элементов.
Не все алгоритмы подходят для решения задачи. Например константный алгоритм a(x) = 1 не подходит.
Это довольно бесполезный алгоритм, который вряд ли принесет пользу сайту.
Поэтому вводится некоторая характеристика качества работы алгоритма — функционал ошибки. Q(a, X)
— ошибка алгоритма a на выборке X. Например, функционал ошибки может быть долей неправильных
ответов. Следует особо отметить, что Q называется функционалом ошибки, а не функцией. Это связано с
тем, что первым его аргументом является функция.
Задача обучения состоит в подборе такого алгоритма a, для которого достигается минимум функционала
ошибки. Лучший в этом смысле алгоритм выбирается из некоторого семейства A алгоритмов.
1.1.4. Решающие пни
Простейшим примером семейства алгоритмов являются решающие пни:
A =
x
j < t
|∀j, t	
.
Здесь квадратные скобки соответствуют так называемой нотации Айверсона. Если логическое выражение
внутри этих скобок — истина, то значение скобок равно 1, в ином случае — нулю.
Алгоритм работает следующим образом. Если значение определенного признака x
j меньше некоторого
порогового значения t, то данный алгоритм возвращает ответ 0 (фильм не понравится), в ином случае — +1
(пользователю фильм понравится).
Решающие пни могут быть использованы для построения сложных композиций алгоритмов.
1.2. Обучение на размеченных данных
1.2.1. Постановка задачи
В этом разделе речь пойдет о том, какие бывают типы задач при обучении на размеченных данных, или
обучении с учителем. Общая постановка задачи обучения с учителем следующая. Для обучающей выборки X = (xi
, yi)
`
i=1 нужно найти такой алгоритм a ∈ A, на котором будет достигаться минимум функционала
ошибки:
Q(a, X) → min
a∈A
.
В зависимости от множества возможных ответов Y, задачи делятся на несколько типов.
2
1.2.2. Задача бинарной классификации
В задаче бинарной классификации пространство ответов состоит из двух ответов Y = {0, 1}. Множество
объектов, которые имеют один ответ, называется классом. Говорят, что нужно относить объекты к одному
из двух классов, другими словами, классифицировать эти объекты.
Рис. 1.1: Задача бинарной классификации
Примеры задач бинарной классификации:
• Понравится ли пользователю фильм?
• Вернет ли клиент кредит?
1.2.3. Задача многоклассовой классификации
Классов может быть больше, чем два. В таком случае имеет место задача многоклассовой классификации.
Рис. 1.2: Задача многоклассовой классификации
Примеры задач многоклассовой классификации:
• Из какого сорта винограда сделано вино?
• Какая тема статьи?
• Машина какого типа изображена на фотографии: мотоцикл, легковая или грузовая машина?
3
1.2.4. Задача регрессии
Когда y является вещественной переменной, говорят о задаче регрессии.
Рис. 1.3: Задача регрессии
Примеры задач регрессии:
• Предсказание температуры на завтра.
• Прогнозирование выручки магазина за год.
• Оценка возраста человека по его фото.
1.2.5. Задача ранжирования
Еще одним примером задачи обучения с учителем является задача ранжирования. Эта задача довольно
тяжелая, и речь о ней в данном курсе не пойдет, но знать о ней полезно. Мы сталкиваемся с ней каждый
день, когда ищем что-либо в интернете. После того, как мы ввели запрос, происходит ранжирование страниц
по релевантности их запросу, то есть для каждой страницы оценивается ее релевантность в виде числа, а
затем страницы сортируются по убыванию релевантности. Задача состоит в предсказании релевантности для
пары (запрос, страница).
1.3. Обучение без учителя
В этом разделе мы обсудим, какие бывают постановки задач машинного обучения, кроме обучения с учителем.
Обучением с учителем называются такие задачи, в которых есть и объекты, и истинные ответы на них.
И нужно по этим парам восстановить общую зависимость. Задача обучения без учителя — это такая задача,
в которой есть только объекты, а ответов нет. Также бывают «промежуточные» постановки. В случае частичного обучения есть объекты, некоторые из которых с ответами. В случае активного обучения получение
ответа обычно очень дорого, поэтому алгоритм должен сначала решить, для каких объектов нужно узнать
ответ, чтобы лучше всего обучиться.
Рассмотрим несколько примеров постановки задач без учителя.
1.3.1. Задача кластеризации
Первый пример — задача кластеризации. Дано множество объектов. Необходимо найти группы похожих
объектов. Есть две основные проблемы: не известно количество кластеров и не известны истинные кластеры,
которые нужно выделять. Поэтому задача решается очень тяжело — здесь невозможно оценить качество
решения. Этим и отличается задача классификации — там тоже нужно делить объекты на группы, но в
классификации группы, а точнее классы, фиксированы, и известны примеры объектов из разных групп.
4
Рис. 1.4: Задача кластеризации
Примеры задач кластеризации:
• Сегментация пользователей (интернет-магазина или оператора связи)
• Поиск схожих пользователей в социальных сетях
• Поиск генов с похожими профилями экспрессии
1.3.2. Задача визуализации
Второй пример — задача визуализации: необходимо нарисовать многомерную (а конкретно, d-мерную) выборку так, чтобы изображение наглядно показывало структуру объектов.
Рис. 1.5: Задача визуализации
Примером задачи визуализации является задача визуализации набора данных MNIST. Этот набор данных
был получен в результате оцифровки рукописных начертаний цифр. Каждый скан цифры характеризуется
5
вектором признаков - яркостей отдельных пикселей. Необходимо таким образом отобразить этот набор данных
на плоскость, чтобы разные цифры оказались в разных ее областях.
1.3.3. Поиск аномалий
Третий пример задачи обучения без учителя — поиск аномалий. Необходимо обнаружить, что данный объект
не похож на все остальные, то есть является аномальным.
При обучении есть примеры только обычных, не аномальных, объектов. А примеров аномальных объектов
либо нет вообще, либо настолько мало, что невозможно воспользоваться классическими методами обучения
с учителем (методами бинарной классификации).
При этом задача очень важная. Например, к такому типу задач относится:
• Определение поломки в системах самолета (по показателям сотен датчиков)
• Определение поломки интернет—сайта
• Выявление проблем в модели машинного обучения.
Все упомянутые задачи не будут обсуждаться в рамках данного курса. Им будет посвящен следующий
курс — «Поиск структуры в данных».
1.4. Признаки в машинном обучении
В этом разделе речь пойдет о признаках в машинном обучении. Существует несколько классов, или типов
признаков. И у всех свои особенности — их нужно по-разному обрабатывать и по-разному учитывать в алгоритмах машинного обучения. В данном разделе будет обсуждаться используемая терминология, о самих же
особенностях речь пойдет в следующих уроках.
Признаки описывают объект в доступной и понятной для компьютера форме. Множество значений j-го
признака будет обозначаться Dj .
1.4.1. Бинарные признаки
Первый тип признаков — бинарные признаки. Они принимают два значения: Dj = {0, 1}. К таковым относятся:
• Выше ли доход клиента среднего дохода по городу?
• Цвет фрукта — зеленый?
Если ответ на вопрос да — признак полагается равным 1, если ответ на вопрос нет — то равным 0.
1.4.2. Вещественные признаки
Более сложный класс признаков — вещественные признаки. В этом случае Dj = R. Примерами таких признаков являются:
• Возраст
• Площадь квартиры
• Количество звонков в call-центр
Множество значений последнего указанного признака, строго говоря, является множеством натуральных
чисел N, а не R, но такие признаки тоже считают вещественными.
1.4.3. Категориальные признаки
Следующий класс признаков — категориальные признаки. В этом случае Dj — неупорядоченное множество.
Отличительная особенность категориальных признаков — невозможность сравнения «больше-меньше» значений признака. К таковым признакам относятся:
• Цвет глаз
• Город
• Образование (В некоторых задачах может быть введен осмысленный порядок)
Категориальные признаки очень трудны в обращении — до сих пор появляются способы учета этих признаков
в тех или иных методах машинного обучения.
6
1.4.4. Порядковые признаки
Частным случаем категориальных признаков являются порядковые признаки. В этом случае Dj — упорядоченное множество. Примеры:
• Роль в фильме (Первый план, второй план, массовка)
• Тип населенного пункта (упорядочены по населенности)
• Образование
Хотя и порядковые, и вещественные признаки упорядочены, они отличаются тем, что в случае порядковых
признаков «расстояние» между двумя значениями признака не имеет смысла. Например, отличие значения
3 от значения 2 может быть не таким существенным, как отличие 1 от 0.
1.4.5. Множествозначные признаки
Множествозначный признак — это такой признак, значением которого на объекте является подмножество
некоторого множества. Пример:
• Какие фильмы посмотрел пользователь
• Какие слова входят в текст
1.4.6. Распределение признака
Далее речь пойдет о проблемах, с которыми можно столкнуться при работе с признаками. Первая из них —
существование выбросов. Выбросом называется такой объект, значение признака на котором отличается от
значения признака на большинстве объектов.
Рис. 1.6: Пример выброса
Наличие выбросов представляет сложность для алгоритмов машинного обучения, которые будут пытаться
учесть и их тоже. Поскольку выбросы описываются совершенно другим законом, чем основное множество
объектов, выбросы обычно исключают из данных, чтобы не мешать алгоритму машинного обучения искать
закономерности в данных.
Проблема может быть и в том, как распределен признак. Не всегда признак имеет такое распределение,
которое позволяет ответить на требуемый вопрос. Например, может быть слишком мало данных о клиентах
из небольшого города, так как собрать достаточную статистику не представлялось возможным.
