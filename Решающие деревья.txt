

Мы начинаем урок, посвященный решающим деревьям. Это семейство алгоритмов, которое очень сильно отличается от линейных моделей и в то же время занимает крайне важную роль в машинном обучении. Итак, пока что мы с вами проходили линейные модели. Они легко обучаются. В случае со среднеквадратичной ошибкой для вектора весов даже есть аналитическое решение. Также очень легко применять к линейным моделям градиентный спуск, который быстро сходится, если с признаками все хорошо, и дает нам точное решение. При этом линейные модели могут восстанавливать только очень простые зависимости из-за того, что у них очень мало степеней свободы, очень мало параметров. Их столько, сколько признаков. В то же время линейные модели можно использовать для восстановления нелинейных зависимостей за счет перехода в спрямляющие пространства, что может оказаться довольно сложной операцией. В то же время линейные модели не очень хорошо отражают то, как люди принимают решения. На самом деле, когда человек хочет понять ту или иную вещь, он будет задавать последовательность из простых вопросов, которые в итоге приведут его к какому-то ответу. Давайте разберем простой пример, который слабо относится к жизни, и поэтому не стоит повторять его дома. Итак, медицинская диагностика. Представьте, что пациент приходит к врачу и спрашивает, что с ним случилось. В это время врач очень простой и он может диагностировать только два заболевания: ангину и грипп. Сначала врач спрашивает, какая температура у пациента. Если она меньше 37 градусов, он говорит, что пациент здоров, если больше 37 градусов, он переходит к следующему вопросу, а именно спрашивает, болит ли у пациента горло. Если оно болит, то врач ставит диагноз «ангина», если не болит, врач говорит, что это грипп. В итоге, задав максимум два вопроса, врач приходит к тому или иному ответу. Другой вопрос о том, насколько качественны эти ответы, но в целом они отражают структуру мышления. Или другой пример для известной задачи определения выживет или нет тот или иной пассажир Титаника. Задача очень неплохо решается вот таким решающим деревом. Давайте разберем его. В первую очередь мы смотрим, какой пол у данного пассажира. Если это женщина, то мы сразу говорим, что она выживет, и этот ответ будет правильным в 73 % случаев. Если это мужчина, то мы смотрим, сколько ему лет. Если девять или меньше, то мы перейдем в следующую ветку, а если больше девяти, то сразу говорим, что пассажир погиб. Если меньше девяти лет, то мы смотрим, сколько родственников у этого пассажира было на борту. Если три или больше, то говорим, что он погиб. Если меньше, то говорим, что он выжил. Итак, мы рассмотрели два примера решающих деревьев. В общем случае это некоторое бинарное дерево, у которого в каждой внутренней вершине записано простое условие. В зависимости от того, верное оно или нет, мы будем идти либо вправо, либо влево от этой вершины. В каждом листе решающего дерева записан некоторый прогноз. Таким образом, мы берем некий объект, стартуем из корня и движемся по дереву, проверяя условие в текущей вершине. В зависимости от его выполнения идем либо влево, либо вправо. В конце концов мы попадаем в лист, в котором записан прогноз, который и выдается в качестве ответа модели. Отмечу, что можно строить и более сложные небинарные деревья, но, как правило, используются именно бинарные. Этого достаточно, чтобы решать большинство задач. Условия во внутренних вершинах также используются крайне простые. Наиболее частый вариант — вот такой. Мы проверяем, находится ли значение j-того признака левее, чем некоторый порог. То есть мы берем у объекта j-тый признак, сравниваем с порогом t, и если оно меньше порога, мы идем влево, если больше порога, мы идем вправо. Это опять же очень простое условие, которое зависит всего от одного признака, но его достаточно, чтобы решать многие сложные задачи. Прогноз в листе будет вещественным, если это регрессия, и он будет пытаться как можно лучше приблизить истинный ответ. Если это классификация, то есть два варианта. Дерево может выдавать либо номер класса (тогда в каждом листе будет записан просто тот или иной класс), либо распределение вероятности на классах. В этом случае в каждом листе будет записан некоторый вектор длины k, если k — это число классов, который будет говорить, насколько вероятно, что объект относится к тому или иному классу. Давайте посмотрим, как выглядят зависимости, которые восстанавливают решающие деревья. Рассмотрим задачу классификации с двумя признаками. В ней три класса, и видно, что решающее дерево может очень неплохо отделить каждый класс от всех остальных. Видно, что разделяющая поверхность каждого класса кусочно-постоянная, и при этом каждая сторона разделяющей поверхности параллельна одной из осей координат из-за того, как именно мы выбрали условия. Каждое условие сравнивает значение ровно одного признака, ровно одной координаты с порогом. В то же время решающее дерево может легко переобучиться. Его можно сделать настолько глубоким, что каждый лист решающего дерева будет соответствовать ровно одному объекту обучающей выборки. В этом случае, если мы запишем в каждом листе ответ соответствующего объекта, мы получим нулевую ошибку на обучающей выборке, но в то же время дерево будет явно переобученным. Вот пример такого дерева. Оно идеально отделило синий от красного класса. Но при этом разделяющая поверхность получилась безумно сложной. Видно, что этот алгоритм переобучился, от него не будет никакого толка на тестовой выборке. Посмотрим теперь на задачу регрессии. Пусть у нас есть всего один признак x, и нужно по его значению восстановить значение целевой переменной y. Если построить не очень глубокое дерево, то оно восстановит зависимость примерно так. Опять же, видно, что восстановленная зависимость будет кусочно-постоянной, но в целом качество довольно неплохое. Если же мы увеличим глубину дерева, то получим вот такую функцию. Видно, что дерево подогналось под выбросы, и его качество будет уже не таким хорошим. Дерево переобучилось из-за того, что его глубина слишком большая. Итак, мы с вами узнали, что такое решающее дерево, и поговорили о том, что оно последовательно проверяет простые условия и приходит в тот или иной лист, где записан прогноз. Деревья интерпретируемы, но при этом позволяют восстанавливать очень сложные зависимости. Из-за этого они довольно легко переобучаются под обучающую выборку. В следующем видео мы продолжим разговор о решающих деревьях и обсудим, как именно можно их обучать.

В этом видео мы поговорим о том, как строить решающие деревья, как обучать их на конкретную выборку. Как мы выяснили в прошлый раз, решающие деревья очень легко переобучаются. Легко построить дерево, у которого каждый лист будет соответствовать одному объекту обучающей выборки. Это дерево будет строить вот такую разделяющую поверхность, очень-очень сложную, которая будет давать идеальное качество, но при этом явно будет переобученной. Поскольку дерево может достичь нулевой ошибки на обучающей выборке, нам не подходит любое дерево. В принципе, скорее всего, каждую задачу можно решить деревом, которое будет иметь нулевую ошибку и при этом не быть переобученным, и, скорее всего, это будет минимальное дерево из всех, у которых нулевая ошибка. Минимальным оно может быть, например, в смысле количества листьев, то есть можно поставить задачу построить такое решающее дерево, которое не ошибается на данной выборке и при этом имеет меньше всего листьев. К сожалению, эта задача NP-полная, то есть ее невозможно решить за разумное время, поэтому в машинном обучении пользуются гораздо более простым подходом: строят дерево жадно, последовательно, от корня к листьям, а именно мы начинаем с пустого дерева, дальше выбираем каким-то образом корень, который разбивает нашу выборку на две, дальше разбиваем потомков этого корня и так далее. Ветвим дерево до тех пор, пока не решим, что этого достаточно. Давайте выясним, как именно можно разбивать конкретную вершину на две, на два потомка. Итак, как мы с вами договаривались, мы будем использовать в качестве условия для разбиения очень простую штуку: будем брать один из признаков, j-тый, и сравнивать его с порогом t. Если значение j-того признака меньше порога, отправляем объект в одну сторону, например, влево, если больше порога, то вправо. Допустим, мы сейчас находимся в вершине m, и в нее попало некоторое количество объектов обучающей выборки, например, xm, будем так обозначать это подмножество объектов. Мы будем использовать некоторый критерий ошибки Q, который зависит от того, какие объекты попали в данную вершину, то есть xm, и от параметров разбиения j и t, то есть на основе какого признака мы разбиваем и с каким порогом мы сравниваем значение этого признака. Будем выбирать параметры j и t-разбиения так, чтобы они минимизировали данный критерий ошибки Q. Подбирать параметр j можно перебором, поскольку признаков у нас конечное число, а из всех возможных значений параметра t, порога, можно рассматривать только те, при которых получаются различные разбиения. Можно показать, что этих значений t столько, сколько различных значений признака j на обучающей выборке. Например, можно отсортировать все значения j-того признака и брать пороги между этими значениями. После того как мы выбрали конкретное разбиение, выбрали оптимальные значения параметров j и t, мы разбиваем нашу вершину на две: левую и правую. При этом часть объектов, а именно те, на которых j-тый признак меньше или равен порогу t, отправляются влево, и будем обозначать это подмножество как xl, а часть объектов из xm, те, у которых значение j-того признака больше порога t, отправляются вправо, и это подмножество обозначается как xr. Эту процедуру можно повторить дальше для двух дочерних вершин, тем самым углубляя наше дерево. В какой-то момент нам все же придется остановиться. Как понять, что данную вершину уже разбивать не нужно, что ее можно объявить листом и выдавать прогноз для того объекта, который в нее попал? Критериев останова очень много. Например, можно смотреть, сколько объектов находится в данной вершине. Если там всего один объект обучающей выборки, понятно, что дальше разбивать не имеет смысла; если же больше, можно разбивать дальше. Или, например, можно смотреть, какие объекты попали в эту вершину: если они все относятся к одному классу в задаче классификации, можно прекратить разбиение; если же есть несколько классов, можно разбивать дальше. Или, например, можно следить за глубиной дерева и останавливать разбиение, если глубина превышает некоторый порог, например, 10. Мы будем подробнее говорить о критериях останова в следующих видео этого урока, а сейчас давайте обсудим, как выбирать ответ в листе, если мы решили вершину объявить листом. Итак, в данный лист попала некоторая подвыборка xm, некоторое подмножество объектов обучающей выборки, и нужно выбрать какой-то один прогноз, который будет оптимален для данной подвыборки. В случае с регрессией мы знаем, что если функционал – среднеквадратичная ошибка, то оптимально выдавать средний ответ по этой подвыборке, то есть мы суммируем ответы по всем объектам i, которые попали в данную вершину, и делим на количество объектов в этой вершине. Это и будет оптимальным прогнозом в случае с задачей регрессии среднеквадратичного функционала. Если мы решаем задачу классификации, то наиболее логичным выбором будет возвращать тот класс, который наиболее популярен в выборке xm. То есть мы для каждого класса y считаем, сколько объектов этого класса попало в данную вершину, и возвращаем тот, который максимален, которого больше всего. Если же мы хотим возвращать вероятности классов в данной вершине, это тоже очень легко сделать. Вероятность k-того класса оценивается как доля объектов k-того класса в данной вершине среди всех объектов, впавших в эту вершину, то есть среди всех объектов из xm. У нас остались открытыми два вопроса: как именно разбивать, как задавать критерии разбиения, как оценивать ошибку разбиения в данной вершине и как выбирать критерий останова? О них мы будем говорить в следующих видео этого урока. Мы обсудили, что решающие деревья удобно строить жадно – от корня к листьям, при этом конкретное разбиение в каждой вершине выбирается, исходя из некого критерия ошибки, о котором будем говорить позже. Также нужно задать некоторый критерий останова, который определяет, следует ли данную вершину разбивать дальше или же уже можно сделать ее листом. Также мы обсудили, как именно выбирать прогнозы в листьях, в задачах классификации и регрессии. В следующем видео мы продолжим разговор и поговорим о том, как задавать критерии ошибки.

В этом видео мы поговорим о критериях информативности, с помощью которых можно выбирать оптимальное разбиение при построении решающего дерева. Как мы с вами договорились, решающее дерево будем строить простое, у которого в каждой вершине записано условие, которое берет значение j-го признака, сравнивает его с порогом t, и если значение признака меньше порога t, то объект идет в левое поддерево, если больше, то в правое поддерево. Допустим, у нас есть некоторая вершина с номером m, в которую попала подвыборка обучающей выборки X с индексом m, и мы хотим разбить эту вершину на два поддерева. Мы уже говорили, что будем делать это с помощью критерия ошибки, который показывает, насколько качественно данное условие, данная пара (признак j и порог t) разбивает объект, попавший в эту выборку, на две подвыборки. Этот критерий обозначается буквой Q. После того, как конкретный критерий выбран, мы разбиваем выборку Xm на две части. Xl — это те объекты, у которых значения j-го признака меньше или равно порога t, и Xr — это те объекты, у которого значения j-го признака больше порога t. Выборка Xl отправляется в левое поддерево, выборка Xr отправляется в правое поддерево. После этого можно повторить процедуру для левого и правого листа, который мы породили из данной вершины. Критерий ошибки будем записывать вот в таком сложном виде. Давайте разберемся, что означают его части. Он состоит из двух слагаемых. В первом слагаемом используется функция H, которой на вход передается выборка Xl, то есть та часть объектов, которая пойдет в левое поддерево. Функция H должна измерять качество этого подмножества, то есть насколько сильный разброс ответов имеет место при попадании выборки Xl в левое поддерево. Аналогично второе слагаемое измеряет то же самое для правого поддерева, в нем функцию H передает выборка Xr, и она должна измерить, насколько силен разброс ответов в подмножестве Xr. Обратите внимание, что значение функции H на Xl и Xr нормируется, они домножаются на коэффициенты, которые равны доли объектов, которая идет влево, и доли объектов, которая идет вправо. Зачем это нужно? Представьте, что у нас в вершине m находится 1000 объектов, и из них 990 идут в левое поддерево, и 10 — в правое поддерево. При этом 990 объектов, которые идут влево, оказываются одного класса, то есть это очень хорошее поддерево, а 10 объектов, которые идут вправо, относятся ко всем возможным классам. Распределение классов там равномерное, то есть эта подвыборка получается плохой, но при этом в ней всего 10 объектов, и нам не так страшно, что она получилась плохой, при том, что 990 попали в правильную вершину. Поэтому нам важно домножать значение качества подмножества на размер этого подмножества. Итак, функция H(X) называется критерием информативности, и она должна измерять, насколько силен разброс ответов в выборке X. По сути, эта функция зависит от того, какие ответы имеют объекты из множества X. Ее значение должно быть тем меньше, чем меньше разброс этих ответов. Давайте разберем несколько примеров критерия информативности для задач регрессии и классификации. Начнем с регрессии. Понятно, что в случае с регрессией измерить разброс довольно просто — это просто дисперсия ответов этой выборки. Чтобы ее измерить, сначала вычислим средний ответ выборки X, который обозначается буквой y с верхней чертой. Он вычисляется просто, как среднее значение. А затем вычислим дисперсию выборки, которая вычисляется как среднее значение квадрата отклонения ответа на объекте от среднего ответа по выборке. Перейдем к классификации. В случае с классификацией все немного сложнее. Нам понадобится вспомогательная величина, которая показывает для k-го класса, какова доля объектов класса k в выборке X. Будем обозначать эту величину как pk-тое. И она, собственно, вычисляется по этой формуле, смысл которой как раз таки доля объектов класса k в выборке X. На основе этих чисел pk вводятся критерии информативности для классификации, и первый из них — это критерий Джини. Он вычисляется по такой формуле. В нем стоит суммирование по всем классам, от первого до K, и для каждого класса вычисляется произведение pk на (1 − pk), где pk — это доля объектов k-го класса в вершине. Обратите внимание, что все числа в этой сумме положительные, поэтому критерий Джини всегда не отрицательный, он не меньше нуля. При этом, если в нашей выборке X все объекты относятся к какому-то одному классу, например к первому, то все слагаемые в этой сумме будут нулевыми, значит и сам критерий Джини будет равен 0. Это означает, что его оптимум достигается в том случае, если все объекты в подвыборке относятся к одному классу. У него есть много интерпретаций. И одна из них следующая: критерий Джини равен вероятности ошибки случайного классификатора, где случайный классификатор устроен так, что он выдает случайный класс от 1 до K, при этом вероятность выдать класс (какое-то k) равна pk, то есть равна пропорции этого класса в общей подвыборке X. Еще один пример критерия информативности для классификации, это энтропийный критерий. Он вычисляется по такой формуле. В нем суммируются следующие слагаемые: берется вероятность pk и домножается на логарифм этой вероятности, и все это берется со знаком минус. При этом мы считаем, что если вероятность равна 0, то ноль умножить на логарифм нуля — это то же ноль. Причем это можно доказать по непрерывности, если взять просто предел функции X In X. Для этого критерия выполнено то же самое свойство: если в выборке X находятся объекты ровно одного класса, например первого, то значение энтропийного критерия будет равно 0. И какое бы не было распределение на классах, значение энтропийного критерия не отрицательное. У него так же есть очень интересный физический смысл. По сути, энтропийный критерий — это мера отличия распределения классов от вырожденного. Если распределение вырожденное, то энтропия равна 0, в этом распределении нет ничего неожиданного, мы всегда знаем, что мы будем получать из него. Если же это распределение равномерное, то есть вероятность получить каждый класс в этой выборке одинаковая, то энтропия будет максимальна. У этого распределения максимальный уровень неожиданности. Мы не можем предсказать, что мы получим. Итак, мы выяснили, что критерий ошибки должен минимизировать разброс ответов в обоих поддеревьях после разбиения по какому-то условию. И выражается он через критерий информативности, который измеряет разброс ответов в одном из поддеревьев. В регрессии это просто среднеквадратичная ошибка или дисперсия. В классификации это может быть критерий Джини или энтропийный критерий. В следующем видео мы поговорим о том, как выбирать критерий останова и что такое стрижка деревьев.

В этом видео мы поговорим о способах борьбы с переобучением решающих деревьев, а именно про критерии останова и про стрижку деревьев. Критерий останова показывает, нужно ли останавливать процесс построения дерева. Например, когда мы разбили дерево до какой-то степени и находимся в определенной вершине, мы хотим понять, нужно ли ее разбивать дальше или же стоит оделить ее листом? Критерий останова должен отвечать на этот вопрос. Как мы с вами уже обсуждали, худший случай решающего дерева — это дерево, у которого каждый лист соответствует одному объекту обучающей выборки. В этом случае дерево будет максимально переобученным, оно не будет обобщать информацию, полученную из обучающей выборки. Критерий останова, грамотно подобранный критерий останова — это способ борьбы с таким переобучением. Самый простой критерий останова проверяет, все ли объекты, которые находятся в данной вершине, относятся к одному классу. Понятно, что это работает только для классификации. Это простой и понятный критерий останова, но при этом он будет выполнятся в нетривиальных случаях только на простых выборках. Если выборка и зависимости в ней сложные, то, скорее всего, этот критерий сработает только тогда, когда в каждом листе останется по одному объекту. Гораздо более устойчивый и полезный критерий проверяет, сколько объектов оказалось в данной вершине. Если их больше, чем n, то разбиение продолжается, а если меньше или равно, чем n, то процесс построения останавливается в этой вершине, и n — это некоторый параметр, который нужно подбирать. Если n = 1, мы получаем худший случай с деревом, где в каждом листе по одному объекту. Выбирать n нужно так, чтобы по n объектам, которые попали в вершину, можно было устойчиво построить прогноз, можно было надежно оценить, какой прогноз выдавать на этих объектах. Существует рекомендация, что n нужно брать равным 5, по идее, при 5 точках, если у нас 5 объектов попали в вершину, можно уже более или менее надежно оценить, какой ответ на них нужно выдавать. Еще один критерий, гораздо более грубый — это ограничение на глубину дерева. Если мы видим, что эта вершина находится на 5-м уровне дерева и максимальная глубина равна 5, мы останавливаем построение независимо ни от чего — ни от распределения классов в этой вершине, ни от числа объектов в ней — просто останавливаем построение. Критерий довольно грубый, но при этом он хорошо себя зарекомендовал при построении композиций, то есть когда мы объединяем много решающих деревьев в один сложный алгоритм. Мы об этом будем говорить позже в этом модуле. Существует и другой подход к борьбе с переобучением деревьев, а именно стрижка. Это заключается в следующем: мы строим решающее дерево максимальной сложности, максимальной глубины, никак не ограничивая его, то есть строим его до тех пор, пока в каждой конечной вершине не окажется по одному объекту обучающей выборки. После этого мы начинаем удалять, стричь листья в этом дереве по некоторому критерию. Например, можно стричь их до тех пор, пока улучшается качество на некоторой отложенной выборке. Существует мнение, и это подкреплено многими экспериментами, что стрижка работает гораздо лучше, чем простые критерии, о которых мы говорили раньше. В то же время стрижка — это довольно трудоемкая процедура. Например, она может требовать вычисления качества дерева на некоторой валидационной выборке на каждом шаге, что может быть очень сложно. На самом деле, деревья сами по себе практически не используются на сегодняшний день, они нужны лишь для построения композиции, для объединения большого числа деревьев в один алгоритм. И в случае с композициями такие сложные подходы к борьбе с переобучением уже не нужны, оказывается достаточно простых критериев останова вроде ограничения на глубину или число объектов в каждом листе. Итак, мы с вами обсудили, что деревья сами по себе получаются переобученные и зачастую нужно бороться с их переобучением. К этому есть два подхода. Первый — использование простых критериев останова, таких как: ограничение на глубину дерева или ограничение на число объектов в каждом листе дерева. Также есть подход, который называется стрижкой деревьев, который дает более высокое качество, но при этом гораздо более сложный в применении. В следующем видео мы поговорим о том, как использовать категориальные признаки в решающих деревьях.

В этом видео мы обсудим, как использовать категориальные признаки в решающих деревьях. Напомню, что до этого мы обсуждали вот такие условия, которые используются в каждой вершине решающего дерева. В них берется значение j-го признака и сравнивается с порогом t. Если значение меньше или равно этого порога, то объект отправляется в левое поддерево, если больше этого порога, то в правое поддерево. Понятно, что это работает только для вещественных и бинарных признаков, поскольку их мы можем сравнивать с некоторыми числами. Для категориальных признаков это не так, поскольку они принимают значения из некоторого неупорядоченного множества, значения, которые мы не можем сравнивать между собой. Рассмотрим подход, который позволяет включить категориальные признаки в деревья. Он состоит в том, чтобы строить не бинарные, а n-арные деревья, у которых из каждой вершины может выходить вплоть до n ребер. Итак, допустим, мы находимся в некоторой вершине m и хотим разбить ее по некоторому признаку. Для вещественных и бинарных признаков все еще будем рассматривать простые условия, которые сравнивают значения j-го признака с некоторым порогом t. Что же делать в случае с категориальными признаками? Итак, допустим, есть j-й признак, который категориальный и принимает n возможных значений, которые будем обозначать, как c1, …, cn, в этом случае мы будем разбивать эту вершину не на две вершины, как было раньше, а на n вершин, каждая будет соответствовать своему значению категориального признака. Соответственно, в i-ю вершину будут отправляться те объекты, на которых значения j-го признака равно ci. Итак, допустим, мы построили такое разбиение и разбили вершину m на n частей, в i-ю часть отправляется множество объектов xi — это те объекты, на которых категориальный признак принимает значение ci. Как оценить ошибку такого разбиения? В случае с вещественными или бинарными признаками, мы вычисляли взвешенную сумму критериев информативности. В этом случае поступим так же, только слагаемых будет не 2, а n штук. Каждое слагаемое устроено аналогично. Мы вычисляем долю объектов из вершины m, которые идут в i-е поддерево и умножаем на значение критерия информативности для подвыборки xi — той, которая отправилась в i-е поддерево, в i-ю дочернюю вершину. И находим такой категориальный признак, для которого эта сумма оказывается минимальной, Итак, мы рассматриваем все вещественные, бинарные и категориальные признаки, которые есть в выборке. Для категориальных мы вычисляем значение критерия ошибки так, как только что обсудили, для вещественных и бинарных так, как обсуждали в прошлых видео. И выбираем тот признак и тот порог, при которых значения критерия ошибки получаются минимальным, и именно по этому признаку и порогу строим разбиение данной вершины. Заметим, что в случае с категориальными признаками мы генерируем больше дочерних вершин, скорей всего в них будет достигаться более высокое качество и будет получаться более низкое значение критерия информативности. Поэтому, скорей всего, при таком подходе предпочтения почти всегда будут отдаваться разбиению по категориальным признакам с большим числом возможных значений. В результате получится очень много листьев в дереве, что, почти гарантировано, может привести к переобучению. Однако это не всегда так. Если у нас выборки очень большие и даже при разбиении по категориальному признаку у нас будет оказываться много объектов в каждом поддереве, то такой подход будет работать очень неплохо, поскольку он будет восстанавливать сложные зависимости и при этом не переобучаться, при использовании должного критерия останова. Есть и другой подход, который не требует строить такие сложные деревья и продолжает работать с бинарными деревьями. Итак, допустим, j-й признак категориальный и нам нужно как-то разбить по нему данную вершину. Построим некоторое разбиение множества значений этого признака C на два подмножества, которые не пересекаются: C1 и C2. В объединении они должны давать все множество значений j-го признака C. После того как такое разбиение построено, а как именно мы его строим, мы обсудим чуть позже, так вот, если оно построено, то условие в данной вершине будет выглядеть просто — оно проверяет, в какое из этих двух подмножеств попадает значение j-го признака на данном объекте. Если оно попадает в подмножество C1 — отправляем объект влево, если в C2 — отправляем его вправо, дерево остается бинарным. Итак, главный вопрос: как же именно разбить C на два подмножества? Всего возможных разбиений — 2 в степени n. Это очень много, мы не можем себе позволить устроить такой перебор, но оказывается это и не нужно. Есть одна хитрость, которая позволяет избежать полного перебора. В чем она заключается? Для начала отсортируем все возможные значения категориального признака, а их n штук, по некоторому принципу. Будем обозначать минимальное значение как C с индексом (1), второе по минимальности, как C с индексом (2), и так далее до C индексом (n) — это будет максимальное значение. После этого заменим i-е, по порядку значение категориального признака, на число i, то есть перейдем от категориального признака к вещественному и дальше будем работать именно с этим вещественным признаком. Строить для него разбиение, просто выбирая порог. Понятно, что это гораздо быстрее. Порогов столько, сколько различных значений, в нашем случае n штук. Поговорим о том, как нужно сортировать значения категориального признака. Это делается по вот такому очень сложному принципу. Давайте разберемся, что здесь написано. Итак, в числители дроби записана сумма по всем объектам из подвыборки xm — это объекты, которые попали в данную вершину. Для каждого объекта мы вычисляем индикаторы, которые будут равны 1, если j-й признак, тот который нас интересует, на данном объекте принимает значение c какое-то конкретное, и при этом сам объект относится к первому классу. Мы говорим о задаче бинарной классификации. В знаменателе записано число объектов подвыборки xn, у которых значения категориального признака равняется c. По сути, смысл этой дроби в следующем: она показывает, какая доля объектов, для которых значения категориального признака равно c, относится к первому классу. Как много объектов первого класса среди тех, у которых значение категориального признака равно c. Дальше мы сортируем по значению этой дроби все значения категориального признака. Следственно, чем меньше объектов для данного значения относится к первому классу, тем левее данное значение будет в этой цепочке, тем меньше будет данное значение в нашем порядке. Для регрессии все вычисляется аналогично. Только на этот раз мы вычисляем не долю объектов положительного класса с данным значением категориального признака, а средний ответ по всем объектам, у которых значение категориального признака равно c. И сортируем по этому среднему значению. При этом, чем меньше среднее значение при данном значении категориального признака, тем левее будет оно в этой цепочке. Главная особенность этого подхода состоит в том, что если бы мы перебирали все возможные разбиения множества C на два подмножества и искали оптимальное, с точки зрения, например, критерия Джини, то при таком подходе мы найдем то же самое разбиение. То есть такой подход позволяет перебирать всего n возможных разбиений вместо 2 в степени n, но при этом среди этих n будет самое оптимальное. Таким образом, благодаря этому трюку мы можем сократить перебор с экспоненциального до линейного. При этом это условие выполнено для критерия среднеквадратичной ошибки, для критерия Джини и для энтропийного критерия. Итак, мы обсудили два подхода к использованию категориальных признаков в решающих деревьях. Первый основан на построении n-арных решающих деревьев, в которых бы разбиваем вершину на столько поддеревьев, сколько значений у категориального признака. Второй подход работает с бинарными деревьями, там мы разбиваем множество значений категориального признака на два подмножества и отправляем объект влево или вправо, в зависимости от того, в какое из подмножеств выпадает значение категориального признака на этом объекте. На этом мы заканчиваем урок про решающие деревья, а дальше будем говорить о том, как строить композиции решающих деревьев, как объединить большое количество решающих деревьев в один сильный алгоритм.

Привет. В этом модуле вы уже многое узнали про деревья решений, а значит настало время научиться их строить. В этом видео мы потренируемя строить деревья решений с помощью модуля tree из библиотеки Sklearn. По ссылкам ниже доступна документация этой библиотеки, а также множество примеров. Для начала давайте импортируем нужную функциональность, Здесь используются все привычные вам модули, из новых только модуль trees. Также мы с вами будем строить много графиков, поэтому мы сразу подключим magic pylab. Теперь можно переходить к генерации данных. Давайте решать задачу многоклассовой классификации, создадим с помощью функции make_classification dataset, состоящий из двух признаков: x и y координаты. Нам так удобней будет его отрисовывать. И сделаем задачу многоклассовой классификации с тремя классами. Итак, генерируем данные, теперь давайте сразу же получившийся набор данных отрисуем. Для этого нам понадобится создать Colormap, нам также понадобится еще один Colormap, когда мы будем строить разделяющие поверхности, поэтому давайте сразу объявим целых два. И теперь отрисуем наши объекты на плоскости. Мы их отрисовали в координатах признаков. В данном случае нам важно расположение каждой точки (мы еще будем к этому обращаться), поэтому сразу сделаем их такими большими. Это делается с помощью атрибута s. Так, точки мы получили. Видим, что у нас есть три облака точек, причем некоторые облака накладываются друг на друга, что только усложняет нашу задачу. Теперь давайте разобъем данные на обучение и тест и перейдем к построению модели. Разбивает данные с помощью функции train_test_split. Итак, теперь давайте строить модель. Для того чтобы построить модель, воспользуемся методом DecisionTreeClassifier из модуля tree, который возвращает нам объект classificator decision tree. Для начала мы с вами создаем объект с парамерами по умолчанию, и сразу же давайте наше дерево обучим с помощью метода fit. Итак, обученное дерево готово. Теперь давайте построим предсказания, для этого применим метод predict и передадим ему на вход тестовую выборку, и сразу же оценим качество с помощью метрики accuracy, передав функции accuracy_score на вход 2 аргумента: test_labels — правильные ответы на нашей тестовой выборке и наши предсказания, которые мы сейчас построим. Итак, видим, что с помощью данного алгоритма мы правильно оцениваем приблизительно 70 % объектов. Ну, довольно неплохо. Итак, мы научились строит базовое решающее дерево, а теперь давайте проанализируем, как меняется качество модели, а также вид разделяющей плоскости в зависимости от параметра дерева, например в зависимости от его глубины. Для того чтобы отрисовать разделяющую плоскость нашего алгоритма, нам понадобится реализовать ряд дополнительных функций. Первая вспомогательная функция будет называться get_meshgrid и на вход будет принимать данные шаг и граница. Давайте рассмотрим, что же она делает. Зная наши данные, мы можем легко оценить, как меняются значения каждого признака на наших объектах. В данном случае у нас всего два признака — x и y координата, поэтому по данным мы легко можем понять, в каких границах меняется x и y. Соответственно, зная эти границы, мы можем получить набор точек, находящихся внутри квадрата по x и y в соответствии с нашими данными. Вот давайте все эти точки получим, будем получать точки с некоторым шагом (пусть он будет 0,05) и вернем некоторый объект под называнием meshgrid — набор наших точек. Именно с помощью этого объекта мы будем отрисовывать разделяющую плоскость. Так. Следующая вспомогательная функция называется plot_decision_surface. Именно она отвечает непосредственно за отрисовку наших графиков. Функция принимает на вход целый ряд аргументов: во-первых, это модель, которую мы анализируем; это обучающая и тестовая выборка (как данные, так и метки); и это два объекта colormap: один нужен для того, чтобы отрисовывать объекты в плоскости признаков, другой нужен для того, чтобы отрисовывать разделяющую поверхность. Пусть наша разделяющая поверхность будет несколько светлее, для того чтобы на ней хорошо было видно объекты. Итак, для того чтобы получить разделяющую поверхность, первое, что нужно сделать — это обучить модель. Делаем это с помощью метода fit. Обучаем модель на обучающей выборке. Далее давайте зададим размер нашего рисунка. Так как нам интересно посмотреть и на обучающие объекты, и на объекты из тестовой выборки, то давайте сразу будем строить subplot. Рисунок будет состоять из двух рисунков, поэтому сделаем наш график чуть шире по горизонтали, чем по вертикали. Так, теперь переходим непосредственно к отрисовке разделяющей поверхности. Для начала рисуем график с обучающими объектами. Нам нужно получить наш meshgrid, делаем это с помощью функции, которую мы определили шагом ранее. Теперь как же нам раскрасить все эти точки в правильные цвета, чтобы наглядно увидеть разделяющую поверхность? Ну вот давайте сделаем следующий трюк: представим, что каждая точка является объектом, который подлежит классификации. Фактически что мы можем сделать? Мы можем взять нашу обученную модель и применить ее к каждой точке на плоскости. Так как эти точки находятся внутри границ изменения признаков, то таким образом мы получим квадрат, на котором легко сможем отобразить все наши обучающие объекты. Вот давайте этот объект получим. Далее с помощью метода predict будем классифицировать каждую из этих точек и таким образом получим набор меток. Эти метки мы будем использовать в качестве цветов для построения разделяющей плоскости. Далее с помощью метода pcolormesh давайте отрисуем нашу разделяющую плоскость. Заметьте, что сюда мы передаем сами точки xx, yy. Также мы передаем наши предсказания. Они нужны будут для того, чтобы отрисовать объекты разными цветами, и указываем, какой colormap мы используем. В данном случае мы используем light_colors — светлые цвета. Далее поверх нашей разделяющей поверхности мы можем отрисовать объекты. Делаем это с помощью уже известной нам функции scatter, и здесь используем другой colormap. Ну и дальше давайте зададим нашему графику название, и прямо в названии запишем качество получившейся модели. Оценим качество с помощью метрики accuracy, используем функцию accuracy_score. Аналогично давайте поступим с тестовыми данными. Так как разделяющая поверхность у нас не изменится, не будем заново ее получать, просто отрисуем еще раз готовую разделяющую поверхность и на ней отметим точки из нашей тестовой выборки. Вот ровно это сделает наша функция. Так, функцию мы определили, теперь давайте ее применим. Для начала создадим очень простое решающее дерево глубины 1. Фактически у нас может быть проверено только одно условие, и дальше должны следовать листья, так как глубина всего лишь 1. Вот давайте создадим такую модель, передадим ее на вход нашей функции plot_decision_surface и посмотрим, как будет выглядеть разделяющая поверхность. Смотрите, мы получили следующий график. Так как дерево глубины 1, очевидно, что мы не сможем использовать 2 признака для классификации. В данном случае выгоднее было использовать признак, разделяющий наши объекты по вертикали, поэтому мы видим, что мы получили соответствующую картинку. Точность классификации составила 66 % на обучающей выборке и 63 % на тестовой. Вот довольно простая разделяющая поверхность. Что если сделать немного более сложную модель, то есть построить дерево глубины 2? Вот давайте посмотрим, как изменится картинка. Создаем соответствующий объект и передаем его нашей функции. Видим, что картинка несколько усложнилась. Теперь мы делим объекты не только по горизонтали, но и по вертикали. Видим, что это больше соответствует нашим данным, и качество действительно растет. Растет качество как на обучении, так и на тесте. Отсюда можно сделать предположение, что глубина деревьев положительно сказывается на качестве, и чем глубже получается наше дерево, тем лучше мы обучаем модель. Ну что ж, давайте, исходя из этого предположения, продолжим увеличивать глубину деревьев. Построим дерево глубины 3. Видим, что качество на обучении продолжает расти, и при этом качество на тесте не падает. Возможно, имеет смысл еще сильнее увеличить глубину дерева? Давайте попробуем. Давайте вообще не будем ничем ограничивать глубину дерева. Будем его строить настолько глубоким, насколько это возможно, так чтобы на обучении не произошло ни одной ошибки. Тогда не будем ее никак ограничивать и посмотрим, что мы получили. Мы видим, что мы получили довольно сложную разделяющую поверхность, там очень много областей нескольких цветов, и качество на обучении равняется 1. Мы не ошиблись ни разу, видим, что все наши объекты находятся в области своего цвета, в правильной области. Теперь смотрим на тестовые данные. Что мы видим? Мы видим, что для тестовых данных такая разделяющая поверхность является неоптимальной, то есть многие точки находятся в чужой области. Как такое могло получиться? Получается, что мы с вами чрезмерно подстроились под обучающие данные, то есть фактически произошло переобучение. Чтобы бороться с проблемой переобучения, мы можем не только ограничивать дерево по глубине, но также накладывать ограничения на другие параметры. Например, давайте поступим следующим образом: ограничим количество объектов, которые необходимы для того, чтобы продолжать ветвление из некоторой вершины. То есть создадим параметр min_samples_leaf = 3. Это означает, что минимальное количество объектов в листе должно быть 3. По умолчанию этот параметр был равен 1. Вот давайте сделаем такое ограничение и посмотрим, как изменится вид нашей разделяющей поверхности. Ну да, видим, что разделяющая поверхность стала несколько проще, чем была на предыдущем графике. Это хорошо. С одной стороны, у нас немножко уменьшилось качество на обучении, но, с другой стороны, выросло качество на тесте, что и говорит о том, что мы построили более хорошую модель. На этом мы с вами заканчиваем урок по решающим деревьям. Мы научились строить этот алгоритм и проанализировали, как параметры дерева влияют на его качество и на вид разделяющей поверхности. На этом мы заканчиваем изучение решающих деревьев, и уже в следующем уроке вы познакомитесь с таким понятием как случайный лес.

Решающие деревья
7.1. Решающие деревья
Решающие деревья — это семейство алгоритмов, которое очень сильно отличается от линейных моделей, но
в то же время играет важную роль в машинном обучении.
7.1.1. Линейные модели (обзор)
До этого момента изучались линейные модели. К особенностям линейных моделей относится следующее:
• Линейные модели быстро учатся. В случае со среднеквадратичной ошибкой для вектора весов даже
есть аналитическое решение. Также легко применять для линейных моделей градиентный спуск.
• При этом линейные модели могут восстанавливать только простые зависимости из-за ограниченного
количества параметров (степеней свободы).
• В то же время линейные модели можно использовать для восстановления нелинейных зависимостей за
счет перехода к спрямляющему пространству, что является довольно сложной операцией.
Отдельно стоит отметить, что линейные модели не отражают особенности процесса принятия решений у
людей. На самом деле, когда человек хочет понять ту или иную вещь, он будет задавать последовательность
из простых вопросов, которые в итоге приведут его к какому-нибудь ответу.
7.1.2. Решающие деревья (пример 1)
Чтобы понять принцип работы решающих деревьев, полезно рассмотреть следующий сильно упрощенный
пример.
Рис. 7.1
Необходимо провести медицинскую диагностику. Врач, который проводит эту диагностику, знает только
2 заболевания — ангина и грипп. Поэтому сначала он спрашивает, какая температура у пациента. Если она
меньше 37 градусов, он заключает, что пациент здоров, в ином случае — переходит к следующему вопросу, а
1
именно, спрашивает, болит ли у пациента горло. Если оно болит, врач ставит диагноз ангина, в ином случае
— грипп.
Создатели курса не рекомендуют серьезно относиться к предложенному методу диагностики заболеваний.
7.1.3. Решающие деревья (пример 2)
Другой пример — для известной задачи определения того, выживет или не выживет тот или иной пассажир
Титаника. Задача очень неплохо решается следующим решающим деревом:
Рис. 7.2
В первую очередь спрашивается пол пассажира. Если это женщина, то решающее дерево сразу заявляет,
что она выживает, и этот ответ верен в 73% случаев, и так далее.
7.1.4. Решающие деревья
Итак, были рассмотрены два примера решающих деревьев, которые представляли собой бинарные деревья,
в каждой внутренней вершине записано условие, а в каждом листе дерева — прогноз. Строго говоря, не
обязательно решающее дерево должно быть бинарным, но как правило используются именно бинарные.
Условия во внутренних вершинах выбираются крайне простыми. Наиболее частый вариант — проверить,
лежит ли значение некоторого признака x
j левее, чем заданный порог t:
[x
j ≤ t].
Это очень простое условие, которое зависит всего от одного признака, но его достаточно, чтобы решать многие
сложные задачи.
Прогноз в листе является вещественным числом, если решается задача регрессии. Если же решается задача
классификации, то в качестве прогноза выступает или класс, или распределение вероятностей классов.
7.1.5. Решающие деревья в задаче классификации
Пусть решается задача классификации с двумя признаками и тремя классами.
2
Рис. 7.3: Использование решающих деревьев в задачах классификации
Видно, что решающее дерево может очень неплохо отделить каждый класс от всех остальных. Видно,
что разделяющая поверхность каждого класса кусочно-постоянная, и при этом каждая сторона поверхности
параллельна оси координат, так как каждое условие сравнивает значение равно одного признака с порогом.
В то же время решающее дерево вполне может переобучиться: его можно сделать настолько глубоким, что
каждый лист решающего дерева будет соответствовать ровно одному объекту обучающей выборки. В этом
случае, если записать в каждом листе ответ соответствующего объекта, на обучающей выборке получается
нулевая ошибка. Дерево получается явно переобученным. Вот пример такого дерева:
3
Рис. 7.4: Переобученное решающее дерево
Это дерево идеально отделило синий от красного класса, но разделяющая поверхность получилась безумно
сложной — видно, что этот алгоритм переобучился и от него не будет никакой пользы на тестовой выборке.
7.1.6. Решающие деревья в задаче регрессии
Пусть решается задача регрессии с одним признаком, по которому нужно восстановить значение целевой
переменной. Не очень глубокое дерево восстанавливает зависимость примерно так:
Рис. 7.5: Использование решающих деревьев в задачах регрессии
Восстановленная зависимость будет кусочно-постоянной, но в целом будет иметь неплохое качество.
При увеличении глубины дерева получившаяся функция будет иметь следующий вид:
4
Рис. 7.6: Переобученное решающее дерево
Видно, что дерево подогналось под выбросы и его качество уже будет не таким хорошим. Дерево переобучилось из-за того, что его глубина слишком большая.
7.2. Обучение решающих деревьев
В данном разделе будет рассмотрен вопрос, как строить решающие деревья и как обучать их по конкретной
выборке.
7.2.1. Переобучение деревьев
В предыдущем разделе было показано, что решающие деревья очень легко переобучаются. В том числе можно
построить дерево, у которого каждый лист будет соответствовать одному объекту обучающей выборки.
Рис. 7.7: Переобученное решающее дерево
Это дерево строит очень-очень сложную разделяющую поверхность и, очевидно, переобучено.
5
Поскольку всегда можно построить такое дерево, которое не ошибается на обучающей выборке и будет
переобученным, имеет смысл искать минимальное (например, с минимальным числом листьев) дерево из имеющих нулевую ошибку. Но, к сожалению, задача отыскания такого дерева – NP-полная, то есть ее невозможно
решить за разумное время.
7.2.2. Жадный способ построения
В машинном обучении применяется жадный способ построения решающего дерева от корня к листьям. Сначала выбирается корень, который разбивает выборку на две. Затем разбивается каждый из потомков этого
корня и так далее. Дерево ветвится до тех пор, пока этого не будет достаточно.
Остается уточнить способ разбиения каждого потомка. Как было сказано ранее, в качестве условия в
каждой вершине строящегося дерева будет использоваться простейшее условие: значение одного из признаков
будет сравниваться с некоторым порогом.
Пусть в вершину m попало множество Xm объектов из обучающей выборки. Параметры в условии [x
j ≤ t]
будут выбраны так, чтобы минимизировать данный критерий ошибки Q(Xm, j, t), зависящий от этих параметров:
Q(Xm, j, t) → min
j,t
.
Параметры j и t можно подбирать перебором. Действительно, признаков конечное число, а из всех возможных
значений порога t можно рассматривать только те, при которых получаются различные разбиения. Можно
показать, что таких значений параметра t столько, сколько различных значений признака x
j на обучающей
выборке.
После того, как параметры были выбраны, множество Xm объектов из обучающей выборки разбивается
на два множества
X` =

x ∈ Xm|[x
j ≤ t]
	
, Xr =

x ∈ Xm|[x
j > t]
	
,
каждое из которых соответствует своей дочерней вершине.
Предложенную процедуру можно продолжить для каждой из дочерних вершин: в этом случае дерево
будет все больше и больше углубляться. Такой процесс рано или поздно должен остановиться, и очередная
дочерняя вершина будет объявлена листком, а не разделена пополам. Этот момент определяется критерием
остановки. Существует много различных вариантов критерия остановки:
• Если в вершину попал только один объект обучающей выборки или все объекты принадлежат одному
классу (в задачах классификации), дальше разбивать не имеет смысла.
• Можно также останавливать разбиение, если глубина дерева достигла определенного значения.
Возможные критерии останова будут обсуждаться позднее в этом уроке.
Если какая-то вершина не была поделена, а была объявлена листом, нужно определить прогноз, который будет содержаться в данном листе. В этот лист попала некоторая подвыборка Xm исходной обучающей
выборки и требуется выбрать такой прогноз, который будет оптимален для данной подвыборки.
В задаче регрессии, если функционал — среднеквадратичная ошибка, оптимально давать средний ответ
по этой подвыборке:
am =
1
|Xm|
X
i∈Xm
yi
.
В задаче классификации оптимально возвращать тот класс, который наиболее популярен среди объектов в
Xm:
am = argmaxy∈Y
X
i∈Xm
[yi = y].
Если требуется указать вероятности классов, их можно указать как долю объектов разных классов в Xm:
amk =
1
|Xm|
X
i∈Xm
[yi = k].
7.3. Критерии информативности
В этом разделе речь пойдет о критериях информативности, с помощью которых можно выбирать оптимальное
разбиение при построении решающего дерева.
6
7.3.1. Выбор критерия ошибки
Критерий ошибки записывается следующим образом:
Q(Xm, j, t) = |X`|
|Xm|
H(X`) + |Xr|
|Xm|
H(Xr)
и состоит из двух слагаемых, каждое из которых соответствует своему листу.
Функция H(X) называется критерием информативности: ее значение должно быть тем меньше, чем меньше разброс ответов в X.
В случае регрессии разброс ответов — это дисперсия, поэтому критерий информативности в задачах регрессии записывается следующим образом:
H(X) = 1
|X|
X
i∈X
(yi − y¯(X))2
, y¯ =
1
|X|
X
i∈X
yi
.
7.3.2. Критерий информативности Джини
Сформулировать критерий информативности для задачи классификации несколько сложнее. Пусть pk — доля
объектов класса k в выборке X:
pk =
1
X
X
i∈X
[yi = k].
Критерий информативности Джини формулируется в терминах pk:
H(X) = X
K
k=1
pk(1 − pk).
Все слагаемые в сумме неотрицательные, поэтому критерий Джини также неотрицателен. Его оптимум достигается только в том случае, когда все объекты в X относятся к одному классу.
Одна из интерпретаций критерий Джини — это вероятность ошибки случайного классификатора. Классификатор устроен таким образом, что вероятность выдать класс k равна pk.
7.3.3. Энтропийный критерий информативности
Еще один критерий информативности — энтропийный критерий:
H(X) = −
X
K
k=1
pk ln pk.
В этом выражении полагается, что 0 ln 0 = 0.
Энтропийный критерий, как и критерий Джини, неотрицателен, а его оптимум также достигается только
в том случае, когда все объекты в X относятся к одному классу.
Энтропийный критерий имеет интересный физический смысл. Он заключается в том, что показывает,
насколько распределение классов в X отличается от вырожденного. Энтропия в случае вырожденного распределения равна 0: такое распределение характеризуется минимальной возможной степенью неожиданности.
Напротив, равномерное распределение самое неожиданное, и ему соответствует максимальная энтропия.
7.4. Критерий останова и стрижка деревьев
В этом разделе речь пойдет о способах борьбы с переобучением деревьев, а именно о критериях останова и
стрижке деревьев.
7
7.4.1. Критерий останова
Критерий останова используется, чтобы принять решение: разбивать вершину дальше или сделать листовой.
Худший случай решающего дерева — такое, в котором каждый лист соответствует своему объекту обучающей выборки. В этом случае дерево будет максимально переобученным и не будет обобщать информацию,
полученную из обучающей выборки. Грамотно подобранный критерия останова позволяет бороться с переобучением.
Самый простой критерий останова проверяет, все ли объекты в вершине относятся к одному классу.
Однако такой критерий останова может быть использован только в случае простых выборок, так как для
сложных он остановится только тогда, когда в каждом листе останется примерно по одному объекту.
Гораздо более устойчивый и полезный критерий проверяет, сколько объектов оказалось в вершине, и
разбиение продолжается, если это число больше, чем некоторое выбранное n. Соответственно, если в вершину
попало ≤ n объектов, она становится листовой. Параметр n нужно подбирать.
Случай n = 1 является худшим случаем, описанным выше. При этом выбирать n нужно так, чтобы по n
объектам, которые попали в вершину, можно было устойчиво построить прогноз. Существует рекомендация,
что n нужно брать равным 5.
Еще один критерий, гораздо более грубый, заключается в ограничении на глубину дерева. Этот критерий
хорошо себя зарекомендовал при построении композиций, когда много решающих деревьев объединяют в
один сложный алгоритм. Об этом пойдет речь позже.
7.4.2. Стрижка деревьев
Существует и другой подход к борьбе с переобучением деревьев — стрижка. Он заключается в том, что
сначала строится решающее дерево максимальной сложности и глубины, до тех пор, пока в каждой вершине
не окажется по 1 объекту обучающей выборки.
После этого начинается «стрижка», то есть удаление листьев в этом дереве по определенному критерию.
Например, можно стричь до тех пор, пока улучшается качество некоторой отложенной выборки.
Существует мнение, и это подкреплено многими экспериментами, что стрижка работает гораздо лучше,
чем простые критерии, о которых говорилось раньше. Но стрижка — очень ресурсоёмкая процедура, так
как, например, может потребоваться вычисление качества дерева на некоторой валидационной выборке на
каждом шаге.
На самом деле, сами по себе деревья на сегодняшний день почти не используются, они бывают нужны
только для построения композиции и объединения большого числа деревьев в один алгоритм. В случае с
композициями такие сложные подходы к борьбе с переобучением уже не нужны, так как достаточно простых
критериев останова, ограничения на глубину дерева или на число объектов в листе.
7.5. Решающие деревья и категориальные признаки
В этом разделе будет рассказано, как использовать категориальные признаки в решающих деревьях.
До этого момента использовалось следующее условие в вершине каждого дерева:
[x
j ≤ t].
Очевидно, что такое условие можно записывать только для вещественных или бинарных признаков.
7.5.1. N-арные деревья
Подход, который позволяет включить категориальные признаки в деревья, состоит в том, чтобы строить nарные деревья, то есть такие деревья, что из каждой вершины могут выходить до n ребер. Пусть необходимо
разбить некоторую вершину по некоторому признаку. Если этот признак — вещественный или бинарный,
то все еще можно использовать простое условие с порогом t, поэтому интерес представляет именно случай
категориального признака.
Если x
j — категориальный признак, который может принимать значения
{c1, ..., cn},
8
можно разбить вершину на n вершин таким образом, что в i-ую дочернюю вершину идут объекты с x
j =
ci
. Критерий ошибки такого разбиения строится по аналогии со случаем бинарного дерева. Поскольку Xn
разбивается на n частей, а не на две, в выражении будет n слагаемых:
Q(Xm, j) = Xn
i=1
|Xi
|
|Xm|
H(Xi) → min
j
.
Таким образом, если вершину m нужно разбить, рассматриваются все возможные вещественные, бинарные
и категориальные признаки. Для вещественных и бинарных признаков считается Q(Xm, j, t), а для n-арных
— так, как написано выше. Разбиение вершины будет происходить по тому признаку, для которого значение
критерия ошибки будет минимальным.
Важное замечание состоит в том, что при делении вершины по категориальному признаку получается
больше дочерних вершин, на которых, очень вероятно, будет достигаться более высокое качество и более
низкое значение критерия информативности, поэтому, скорее всего, при таком подходе предпочтение почти
всегда будет отдаваться разбиению по категориальным признакам с большим числом возможных значений.
В результате получается много листьев в дереве, что почти гарантированно может привести к переобучению.
Однако это не всегда так. Если выборки настолько большие, что даже после разбиения по категориальному
признаку в каждом поддереве будет оставаться много объектов, то такое дерево будет неплохо работать,
поскольку оно будет восстанавливать сложные зависимости, и при этом не переобучаться при использовании
должного критерия останова.
7.5.2. Бинарные деревья с разбиением множества значений
Другой подход позволяет не переходить к n-арным деревьям и продолжать работать с бинарными деревьями.
Пусть также необходимо сделать разбиение вершины m, а категориальный признак x
j может принимать
значения C = {c1, ..., cn}.
Для этого сначала необходимо разбить множество значений категориального признака на два не пересекающихся подмножества:
C = C1 ∪ C2, C1 ∩ C2 = ∅.
После того, как такое разбиение построено, условие в данной вершине будет выглядеть просто:
[x
j ∈ C1]
Это условие проверяет, в какое из подмножеств попадает значение признака в данный момент на данном
объекте. Главным вопросом остается то, как именно нужно разбивать множество C.
Полное количество возможных разбиений множества на два подмножества — 2
n. К счастью, есть хитрость,
которая позволяет избежать полного перебора и, более того, работать с категориальным признаком как с
вещественным. Для этого возможные значения категориального признака сортируются специальным образом
c(1), ..., c(n) и заменяются на натуральные числа 1,...,n. После этого с данными признаком следует уже работать
как с вещественным, а значение порога t будет определять разделение множества C на два подмножества.
Сортировать значения категориального признака в случае задачи бинарной классификации нужно по
следующему принципу:
P
i∈Xm
[x
j
i = c(1)][yi = +1]
P
i∈Xm
[x
j
i = c(1)]
≤ ... ≤
P
i∈Xm
[x
j
i = c(n)
][yi = +1]
P
i∈Xm
[x
j
i = c(n)
]
Фактически, значения категориального признака сортируются по возрастанию доли объектов +1 класса среди
объектов выборки Xn с соответствующим значением этого признака.
Для задачи регрессии сортировка происходит похожим образом, но вычисляется не доля объектов положительного класса, а средний ответ по всем объектам, у которых значение категориального признака равно c:
P
i∈Xm
[x
j
i = c(1)]yi
P
i∈Xm
[x
j
i = c(1)]
≤ ... ≤
P
i∈Xm
[x
j
i = c(n)
]yi
P
i∈Xm
[x
j
i = c(n)
]
.
Главная особенность такого подхода состоит в том, что полученный результат полностью эквивалентен результату, который можно было бы получить в результате полного перебора. Это условие работает для критерия
Джини, MSE и энтропийного критерия.


